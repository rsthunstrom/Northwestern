---
title: "Midterm"
author: "Reed Thunstrom"
subtitle: Predict 413 | Section 59
output: word_document
---

### Problem

Given a set of daily rainfall data, the objective is to predict the next 24 months of precipitation.  The rainfall data is vast, with approximately 66 years of historical data.  These 66 years of daily rainfall data will be used to predicted the next 24 months of rainfall.

### Significance

The problem is an interesting one as much of the world's production comes from rain water.  Farmers no doubt are very interested in monthly rainfall as it provides the fuel for their crops to grow.  Knowing what the precipitation will be is extremely significant and important for them as they need to supply the water themselves if the crops don't get enough water from the rainfall. Certain humans live on the supply of rainwater.  Having an accurate prediction of the upcoming rainwater totals is of paramount importance as the supply is vital to their existence.  Most likely, the exact amount of inches isn't critically important, but some reasonable estimate is no doubt critical for those that rely on the supply of rainwater.

### Data

The data used in the problem is comprised of daily historical precipitation data that spans September, 1946 to July, 2014. The rainfall data is from an unknown location, so we are left with only the daily values without any context to create our predicion.  The dataset is a simplistic one in that it is made up of two columns, the date, and a daily precipitation total.

```{r}
comp_data<- read.csv('/Users/z013nx1/documents/competitiondata.csv', header=TRUE ,sep = ",")
head(comp_data)
```

The dataset is made up of 24,806 observations which can be aggregated into 815 months.  We will use these 815 monthly precipitation totals for our model building.  A plot of the monthly graph shows how sporadic the data is.

```{r}
comp_data$DATE<- as.Date(as.character(comp_data$DATE), "%Y%m%d")
comp_data["Year"] <- as.numeric(substr(as.character(comp_data$DATE),1,4))
comp_data["Month"] <- as.numeric(substr(as.character(comp_data$DATE),6,7))
comp_data_month_year<- (aggregate(comp_data[,'PRCP'], list(comp_data$Month, comp_data$Year), sum))
df<- data.frame(comp_data_month_year)
comp_data_df<- setNames(df, c("Month", "Year", "PRCP"))
```

```{r}
plot(comp_data_df$Year, comp_data_df$PRCP, xlab = "Year", ylab = "Precipitation", main = "Monthly Rainfall:September, 1946 to July, 2014")
```

As we can see in the graph, the data is varied and the majority of the values fall between 0 and 10.  Since this graph is by year, we are missing visibility into how each particular month behaves.  No doubt this will be valuable information when we are attempting to predict future month's rain as it's probably safe to assume that we witness different precipitation totals in different months (ie April vs. October).  In order to do this, we will create a time series data set by month, and create a season plot to see if we can identify any monthly trends.

```{r}
monthly_ts_all<- ts(comp_data_df$PRCP, frequency = 12, start = c(1946, 9))
comp_data_12yr<- comp_data_df[671:815, c("Month", "Year", "PRCP")]
monthly_ts_12yr<- ts(comp_data_12yr$PRCP, frequency = 12, start = c(2002, 7))
```

```{r echo=FALSE}
install.packages("forecast", repo="http://cran.us.r-project.org")
library(forecast)
```
```{r}
seasonplot(monthly_ts_12yr,ylab="Precipitation", xlab="Year", 
  main="Seasonal plot: Monthly Precipitation from July, 2002 to July, 2014", col=1:12)
```

(We've limited the data set here so that the graph is readable)  Examining the graph, it does seem that there are some months with less rainfall than others.  For example, January and Februray don't have any values above ~ 5 inches of rainfall, while May - July (spring time) have some very high recordings of monthly precipitation.  The interesting observation here is that even though we witness more rainfall in the spring time, there is still a large variation of rainfall totals across each of the months.  For example, July has recorded precipitation near 0 inches and precipitation above 15.  Additionally, some years May has the highest precipitation and other years July or September have the highest recorded precipitation.  So while there is some seasonal impact, it's hard to ascertain what exactly it is. (Hyndman & Athanasopoulos, Section 2.1)

```{r}
monthplot(monthly_ts_12yr,ylab="Precipitation",xlab="Year",xaxt="n",
  main="Seasonal deviation plot: Monthly Precipitation from July, 2002 to July, 2014")
axis(1,at=1:12,labels=month.abb,cex=0.8)
```

The above seasonal plot portrays the mean value across all years by month along the horizontal lines and the range of values on the vertial line.  Here we can see the same trend that overall there are some patterns by month as visualized by the mean, but there is still a large variation across any month. (Hyndman & Athanasopoulos, Section 2.1) 

Let's examine the seasonal and trend components of the precipitation data.  We can use the decompose function in the forecast package.  This will split up the variaion in precipitation across a season effect, a trend effect, and a random effect.

```{r}
monthly_ts_dc<- decompose(monthly_ts_12yr)
plot(monthly_ts_dc)
```

We notice a few things from this decomposition.  First that there appears to be a fairly sporadic but cyclical trend.  Generally, we notice a two year increase in trend then a two year decrease in trend.  Signaling that there appears to be some relationship with the precipiation totals and a two year cycle.  Next we notice, that there is clearly a seasonal trend per year.  Less precipitation in the beginning of the year, the most in the middle, and less at the end of the year. This analysis will be helpful as we chose a model to forecast precipitation.  Models that handle seasonality and trend should be favored.

## Models

The three types of model classes that we will build for this forecasting exercise are a multiple regression with season and trend as predictor variables, seasonal naive model, Holt-Winters model, and ARIMA model.  All of these models handle seasonality and trend so they should be fairly accurate when we evalue the predictions on a test set.  

The multiple regression model uses the trend and seasonal effects that are found in the observations (Hyndman & Athanasopoulos, Section 5.1).  Much like a traditional multiple regression we are using predictor variables and attempting to predict the dependent, or the forecasted amount.  

Next we use a seasonal naive model, which looks at the value that was observed in the same period but the last season (Hyndman & Athanasopoulos, Section 2.3).  This model is susceptible to extreme values, so we'll need to monitor the error we are seeing in the test set after forecasting.

After we have examined the multiple regression and seasonal naive models, we will then attempt a Holt Winters prediction.  The Holt Winters model using a forecasting equation with 3 smoothing equations.  One for the level, one for the trend, and one for the seasonal impact (Hyndman & Athanasopoulos, Section 7.5)  There are various hyperparameters of the model that we will examine when we build models with this technique.

Lastly, we will look at an ARIMA.  An ARIMA is an acronym for AutoRegressive Integrated Moving Average.  The combination of the autoregressive and moving average part of the model can be tuned to create the best forecasts.  (Hyndman & Athanasopoulos, Section 8.5)  Again, we will tune the hyperparameters of this model to create the forecast with the lowest error.

## Formulations, Preformance, and Accuracy

### Multiple Regression Model

Using the tslm function in the forecast package we fit our model on the training datset which is all the data points in the time series except the last 24 and then forecast the next 24 points to compared to our test set.  The difference between the forecasted points and the test set will create the error which will we use to assess our model.

```{r}
train <- window(monthly_ts_all, start=c(1946, 9), end=c(2012, 7))
test <- window(monthly_ts_all,start=c(2012, 8))
mynorm <- rnorm(10000, mean=0, sd=1)
mr_fit<- tslm(train ~ trend + season)

```

The following actuals and predictions are shown in the graph below:

```{r}
plot(train, xlab="Year", ylab="Monthly Precipitation", main="Monthly Precipitation: September, 1946 to July, 2012")
lines(fitted(mr_fit), col=2)
legend("topright", lty=1, col=c(1,2), legend = c("Actual", "Predicted"))
```

We can see that the predicted values are following the same pattern but does not have any of the spikes captured where the monthly precipitation values are very high.  Additionally, we notice that the trend of our predicted values is mostly flat but ever so slightly increasing.

To assess if our multiple regression holds the assumptions of an ordinary least squares (OLS) regression, we can examine the residuals to see if they are normally distributed.  A normal distribution of residuals implies that we don't have any bias in our predictions based on some other factor.  For example, we aren't overpredicting all values that are in the month of October.  A simple histogram of the residuals with a normal distribution overlaid shows us that the residuals are behaving normally.

```{r}
mr_res<- mr_fit$residuals
mr_res<- na.trim(mr_fit$residuals)

plotForecastErrors <- function(forecasterrors)
  {
     # make a histogram of the forecast errors:
     mybinsize <- IQR(forecasterrors)/4
     mysd   <- sd(forecasterrors)
     mymin  <- min(forecasterrors) - mysd*5
     mymax  <- max(forecasterrors) + mysd*3
     # generate normally distributed data with mean 0 and standard deviation mysd
     mynorm <- rnorm(10000, mean=0, sd=mysd)
     mymin2 <- min(mynorm)
     mymax2 <- max(mynorm)
     if (mymin2 < mymin) { mymin <- mymin2 }
     if (mymax2 > mymax) { mymax <- mymax2 }
     # make a red histogram of the forecast errors, with the normally distributed data overlaid:
     mybins <- seq(mymin, mymax, mybinsize)
     hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
     # freq=FALSE ensures the area under the histogram = 1
     # generate normally distributed data with mean 0 and standard deviation mysd
     myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
     # plot the normal curve as a blue line on top of the histogram of forecast errors:
     points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}

plotForecastErrors(mr_res)
qqplot(mynorm, mr_res, xlab = "Normal", ylab = "Multiple Regression Residuals", main = "QQ Plot of Multiple Regression Residuals")

acf(mr_res, lag.max = 20)
```

The residuals are mostly centered around zero, but the shape of the distribution isn't exactly symmetrical.  Examining the QQ plot we can see that we have some curvature in our plot which is indicating that the residuals are not exactly normally distributed.  When the residuals are not normally distributed, it brings is uncertainty when we are trying to predict using this model outside of our test set.  Additionaly, we plot the autocorrelation of the residuals to see if there is a correlation between successive predictions.  (Hyndman & Athanasopoulos, Section 2.2) If there is a relationship, then most likely the model can be improved upon.  The correlogram shows that there is minimal autocorrelation between the residuals and only one of the lagged values falls outside of the barrier.  Therefore, we say that we're doing a decent job with this model.

Next we will forecast the next 24 points and see how they compare to our test dataset and measure the error.

```{r}
fcast <- forecast(mr_fit, h=24)
plot(fcast, main="Forecast of Monthly Precipitation using Multiple Regression")
lines(test, col = "green")
legend("topright", lty=1, col = c("blue", "green"),
       legend=c("Forecast","Test"))
accuracy(fcast, test)
```

Again we see the same pattern as we did with the predictions in-sample, that they are not capturing the extremely high values.  They do however capture the trend, so we have modeled the relationship of the time series, just not the magnitude with respect to extreme values.

Additionally, we can see that the Root Mean Squared Error (RMSE) is 2.417614.  This value will be used to compare all of the models that we build (Hyndman & Athanasopoulos, Section 2.6).  The root mean squared error takes the square root of the mean of the squared residuals to determine the amount of error in the model with respect to the forecasted points and the test set.  

Now let's fit the model to the entire dataset, not just the training set.  Then we can use that model to predict the next 24 months.  

```{r}
#Use entire dataset for predicions
mrfit<- tslm(monthly_ts_all ~ trend + season)
fcast <- forecast(mrfit, h=24)
mr_pred<- fcast$mean
mr_pred
```

The outputed point estimates are used as our predictions for the next 24 months.  If we select this model, these will be the values that we use for our submission.  Looking through the values, they are qualititatively the right values, so we confirm that the model is without syntax errors.  

### SEASONAL NAIVE MODEL

The next model that we will chose to explore is a seasonal naive model.  A seasonal naive model uses the previous' seasons data points and applies that to the next season in the time period.  First we build the seasonal naive model.

```{r}
snaivefit <- snaive(train,h=24)
```

Using the snaive function in the forecast package we fit our model on the same training datset declared above and forecast the next 24 points.  The following actuals and predictions are shown in the graph below

```{r}
plot(train, xlab="Year",ylab="Monthly Precipitation Totals", main="Monthly Precipitation: September, 1946 to July, 2012")
lines(fitted(snaivefit), col=2)
legend("topright", lty=1, col=c(1,2), legend = c("Actual", "Predicted"))
```

Here we see that the seasonal naive model does a great job of capturing the wild values as opposed to the previous multiple regression model.  However, this model works well if the same patterns happen season over season and can be extremely detrimental when they don't.  For example a record high precipitation total in 2014 for May would produce an equally high prediction for 2015 when in fact it's probably unlikely to have another record the following year.

To assess how well we fit this model, we can examine the residuals to see if they are normally distributed.  Again, a normal distribution of residuals implies that we don't have any bias in our predictions based on some other factor.  The histrogram of the residuals superimposed on a normal distribution allows us to see this relationship.

```{r}
snaive_res<- snaivefit$residuals
snaive_res<- na.trim(snaivefit$residuals)
plotForecastErrors(snaive_res)
qqplot(mynorm, snaive_res, xlab = "Normal", ylab = "Seasonal Naive Residuals", main = "QQ Plot of Season Naive Residuals")

acf(snaive_res, lag.max = 20)
```

The residuals are centered around 0, and the shape is in line with a normal distribution.  So the first residual test passes however we get a different story when we look at the autocorrelation of the residuals.  Here we see 6 lagged residuals that fall outside our bounds for autocorrelation, meaning that our consecutive residuals are in some way related to each other.  This is a sign that this model will not predict very well and we have yet to account for some factor within the data.

```{r}
plot(snaivefit, plot.conf=FALSE,
  main="Forecast of Monthly Precipitation using Seasonal Naive")
lines(test, col = "green")
legend("topright", lty=1, col=c("blue", "green"),
  legend=c("Seasonal naive method", "Test"))
```

The forecasts are capturing the magnitude that we did not get with the multiple regression model, however since the residuals have so much autocorrelation, our predictions won't be as accurate.

```{r}
accuracy(snaivefit, test)
```

As we thought, the RMSE for this model is substantially higher than the multiple regression model at 2.938719.  

Last, we'll fit the model on the entire dataset and create our forecasts for the next 24 months.

```{r}
#Use entire dataset for predicions
snaivefit<- snaive(monthly_ts_all, h = 24)
snaive_pred<- snaivefit$mean
snaive_pred
```

If we were to select this model we would use those values in our submission.  However, it is unlikely at this point that we would chose it given it's autocorrelation of residuals and it's high RMSE.

### HOLT WINTERS MODEL

The holt winters model comprises the forecasting equation and uses 3 components, the trend, season, and random effect.  In our case the seasons are monthly and the trend is the overall trend for the data set.  Decomposing this on a smaller dataset we can see how they are generated.

```{r}

monthly_ts_12yr<- ts(comp_data_df$PRCP, frequency = 12, start = c(2002, 7))

monthly_ts_dc<- decompose(monthly_ts_12yr)
plot(monthly_ts_dc)
```

From this decomposed graph we can see the trend is cyclical but not trending up or down in any meaninful fashion.  Additionally, we can witness a seasonal effect that appears to be looping every year for each motnh.  

Using the same training and testing data sets, we can apply the Holt Winters model in the forecast package.  We do need to transform our 0 values as the model does not accept them.  So we will impute a small positive values for the 6 observations that are 0 for monthly precipitation totals.

```{r}
train[train==0] <- 0.001
test[test==0] <- 0.001
monthly_ts_all[monthly_ts_all==0] <- 0.001
```

In this Holt Winters model function there are a few parameters that we can adjust.  So rather than attempting to guess at which value we should chose, let's run all iterations of the model hyperparameters and see which model produces the lowest RMSE on our test set.  Then we can choose that version of the Holt Winters model.

```{r}
getHW <- function(s, d) {
  forecast <- hw(train, seasonal = s, damped = d, h = 24)
  return(accuracy(forecast,test)[2,"RMSE"])
}

getHW("additive", TRUE)
getHW("additive", FALSE)
getHW("multiplicative", TRUE)
getHW("multiplicative", FALSE)
```

Here we can see that the model with the lowest RMSE is the model with an additive seasonal effect and a non dampened trend.  We will use this version of the model for our forecasts.  

```{r}
forecastsbest<- hw(train, seasonal = "multiplicative", damped = TRUE, h = 24)
plot(forecastsbest,ylab="Forecast of Monthly Precipitation using Holt Winters",
     plot.conf=FALSE, type="o", fcol="white", xlab="Year")

lines(forecastsbest$mean, type="o", col="blue")
lines(test, type = "o", col = "green")
legend("topright",lty=1, pch=1, col=c("blue", "green"), 
  c("Holt Winters' Multiplicative and Dampened Trend", "Test"))
```

Now that we have built our forecast for the test set, it's examine the residuals and see if they are normally distributed and if we are seeing any autocorrelation.

```{r}
hw_res<- forecastsbest$residuals
plotForecastErrors(hw_res)
qqplot(mynorm, hw_res, xlab = "Normal", ylab = "Holt Winters Residuals", main = "QQ Plot of Holt Winters Residuals")
acf(hw_res, lag.max = 20)
```

Judging by the histrogram of residuals and the QQ plot, it appears that the residuals are essentially normal.  Additionally, the residuals are showing no autocorrelatoin outside the accepted bounds except for the lagged values at 1.6.  This is affirming the use of the model in it's current set up.

Lastly, let's examine the error with respect to the test set.

```{r}
accuracy(forecastsbest, test)
```

Given the normality of residuals and no autocorrelation we find a small RMSE here at 2.384303.  This is the smallest RMSE that we have created so far.  Given these facts, this model is the most desirable so far.

Next we will run the model on the entire dataset and get the predictions that will potentially be used in the submission.

```{r}
forecastsbest<- hw(monthly_ts_all, seasonal = "multiplicative", damped = TRUE, h = 24)
hw_pred<- forecastsbest$mean
hw_pred
```

These are the predicted values using the Holt Winters model and are the most appealing thus far.

### ARIMA

The last model that we will explore is the ARIMA model.  The ARIMA is a combination of an autoregressive model and a moving average model. ARIMA model works best with stationary data or if it's not stationary, then we need to difference it.  First let's check to see if it's stationary or not by completing an Augmented Dickey-Fuller Test. (Hyndman & Athanasopoulos, Section 8.5)

```{r}
install.packages("tseries",repo="http://cran.us.r-project.org")
library(tseries)
adf.test(train, alternative = "stationary")
```


Since the p value is very small, we reject the null hypothesis and state that the data is stationary and hence we do not need to transform it.  Since the ARIMA model has a variety of parameters, we can run through a variety of iterations in order to find the model that produces the lowest RMSE on our testing dataset and select that model for evaluation.

```{r}
getrmse <- function(h,...)
{
  fit <- Arima(train,...)
  fc <- forecast(fit,h=h)
  return(accuracy(fc,test)[2,"RMSE"])
}

getrmse(h=24,order=c(3,0,0),seasonal=c(2,1,0))
getrmse(h=24,order=c(3,0,1),seasonal=c(2,1,0))
getrmse(h=24,order=c(3,0,2),seasonal=c(2,1,0))
getrmse(h=24,order=c(3,0,1),seasonal=c(1,1,0))
getrmse(h=24,order=c(3,0,1),seasonal=c(0,1,1))
getrmse(h=24,order=c(3,0,1),seasonal=c(0,1,2))
getrmse(h=24,order=c(3,0,1),seasonal=c(1,1,1))
getrmse(h=24,order=c(4,0,3),seasonal=c(0,1,1))
getrmse(h=24,order=c(3,0,3),seasonal=c(0,1,1))
getrmse(h=24,order=c(4,0,2),seasonal=c(0,1,1))
getrmse(h=24,order=c(3,0,2),seasonal=c(0,1,1))
getrmse(h=24,order=c(2,1,3),seasonal=c(0,1,1))
getrmse(h=24,order=c(2,1,4),seasonal=c(0,1,1))
getrmse(h=24,order=c(2,1,5),seasonal=c(0,1,1))
```

Iterating through all of the different model hyperparameters we select the model with the lowest RMSE on the test set.  In this case it is:

getrmse(h=24,order=c(4,0,3),seasonal=c(0,1,1))

Using that model, we fit the model and evaluate the performance.

```{r}
arima_fit<- Arima(train, order=c(4,0,3), seasonal=c(0,1,1))
forecastbest<- forecast(arima_fit, h=24)
plot(forecastbest, ylab = "Forecast of Monthly Precipitation using Arima")
lines(test, type = "o", col = "green")
legend("topright",lty=1, pch=1, col=c("blue", "green"), 
  c("ARIMA [4,0,3], [0, 1, 1]", "TEST"))
```

As we've witnessed with the other models, the trend of the predictions is accurate, but the magnitude is not captured.  The prediction intervals fall below 0, which is obviously not possible.  So the predictions capture the essence of the graph but do not have a tight prediction window. 

Examining the residuals we will see if the model is predicting well enough.

```{r}
tsdisplay(residuals(arima_fit))
arima_res<- residuals(arima_fit)
plotForecastErrors(arima_res)
qqplot(mynorm, hw_res, xlab = "Normal", ylab = "ARIMA Residuals", main = "QQ Plot of ARIMA Residuals")
```

The residuals appear to be normally distributed and centered around zero.  The QQ plot shows a relatively normal distribution so we can be fairly confident in the predictions from this model.

Lastly, examining the autocorrelation graphs we witness for the most part the residuals are "white noise", with only a few observations falling outside the threshold.  

```{r}
accuracy(forecastbest, test)
```

Generating the RMSE for the forecast given the test set (which we have already done, but replicating again.)  The RMSE = 2.392237 which is just ever so slightly lower than the Holt Winters model.  Both models appear to be useful and very close.

Finally, we will fit the model with the full dataset and get the predictions for the next 24 months.

```{r}
arima_fit<- Arima(monthly_ts_all, order=c(4,0,3), seasonal=c(0,1,1))
arima_fc<- forecast(arima_fit, h = 24)
arima_preds<-arima_fc$mean
arima_preds
```

These ARIMA predictions or the Holt Winters predictions will be the final selected models based on the testing set in the submission forum.  

## Literature

### Literature 1
[link] (http://facta.junis.ni.ac.rs/eao/eao201104/eao201104-09.pdf)
In this paper, Lepojevic discusses using the holt winters method of forecasting energy consumption.  This is very similar to the rainfall prediction as they are both used for assessing how much resources are available.

### Literature 2
[link] (http://www.dtic.mil/dtic/tr/fulltext/u2/a473648.pdf)
A study of healthcare forecasting needs.  Needs including staffing, purchasing, and healthcare delivery

### Literature 3
[link] (https://labs.omniti.com/people/jesus/papers/holtwinters.pdf)
Example of forecasing using Holt Winters.  They explore the additive, multiplicative seasonality as well as dampened and undampened trend.

### Literature 4
[link] (http://aje.oxfordjournals.org/content/163/2/181.short)
Dushoff discusses using multiple regression for forecasting influenza rates in the United States

### Literature 5
[link] (https://arxiv.org/pdf/1508.07534.pdf)
The paper utilizes an ARIMA to forecast the exchange rates of multiple currencies.


## Limitations

With any forecasting technique we are reliant on the historical data and that it in some way it will repeat the trend that it's shown in the past.  For the seasonal naive model, it's literally predicting the exact same point from the prior period.  So any fluxation from that point will cause a large forecasting error.  That methodology is very susceptible to extreme values.  The Holt Winters model and ARIMA model are also reliant on the previous data.  So for example, if el nino created extreme weather conditions, any predictions going forward will undoubtedly include that el nino effect when in reality el nino is not as cyclical as the next prediction period.

## Future Work

As a next step for these models, I would love to see what techniques other members of the class used to create their forecasts.  Since this is my first exposure to forecasting, I would love to learn some of the "tricks of the trade" when creating reliable and accurate forecasts. Additionally, something that could be useful is to exclude rare weather patterns from the data sets (ie el nino) and see if the error rates come down.  Also, I think it could make sense to forecast daily precipitations instead of monthly and roll up the daily values to arrive at monthly predictions.  I wonder if some of the daily nuances will create a more accurate forecast.  I would also like to explore using a box cox transformation with the regression model to see how the transformed model compares to the non transformed data.  

## Learning

One of the more shocking things I learned throughout the course of this exercise was how similiar the results were across forecasting methods.  I plotted the forecasted values for each model and was surprised how similar they were.  Another thing I learned from reading the text and code examples in Forecasting: Principles and Practice book was how to create mutliple models to assess the error, rather than trying to just select what one would think would be the best model.  Tweaking the hyperparameters of the model and using the model with the least error was a quick and easy way to figure out which hyperparameters to use within the model function. 

## Bibliography

Hyndman, R. J., & Athanasopoulos, G. (n.d.). Forecasting: Principles and practice. Retrieved October 25, 2016, from https://www.otexts.org/fpp

