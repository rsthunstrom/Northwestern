---
title: "PRED422_Final"
author: "Reed Thunstrom"
subtitle: Predict 422 | Section 58
output: word_document
---

## Introduction

The purpose of this report is to aid a charitable organization with their direct marketing efforts.  The charitable organiziation has a list of potential donors with the history of the past donations, this will be the dataset used for the analysis.  The data set consists of 21 features or attributes about the potential donors as well as if the donor has donated in the past, and if so how much.  We will separate the file into a training dataset used to learn the models and a validation dataset which will be used to assess the predictive accuracy of each model.  This analysis will consists of two parts: the first is a classification model used to predict if a target of the direct marketing will donate and a regression model to predict the amount of donation if they were to donate.  These two components will be invaluable to the charity organiziation as they will be able to optimize their mailing for maximum net donations after considering the cost of the mailing.  

Each donor, on average, donates \$14.50 and each piece of direct marketing costs \$2.00. We will use these two metrics in our analysis to define an optimal mailing which will produce the maximum net donations for the charitable organization.  Now that we have describe the goal of the analysis, let's start to expolore the data that is available. 

## Exploratory Data Analysis (EDA)

First let's load in our data and take a look at the first few rows of data. 

```{r setup, include=FALSE}

charity <- read.csv('/Users/z013nx1/Documents/charity.csv') # load the "charity.csv" file
head(charity)

```

We notice that we have 24 columns, 1 ID, 20 features, 1 classification dependent variable (DONR), 1 regression dependent variable(DAMT), and 1 column seperating the file into our training, testing, and validation datasets.  Let's split up of file into the three modeling subsets and start to explore the feature set.

```{r include=FALSE}
data.train <- charity[charity$part=="train",]
data.valid <- charity[charity$part=="valid",]
data.test <- charity[charity$part=="test",]
length(data.train[,1])
length(data.valid[,1])
length(data.test[,1])
```

The training dataset has 3,984 observations and we will validate our models on the validation set which consists of 2,018 observations.  The testing set that we will deploy our final models on has 2,007 observations and we do not have any values for DONR, the binary classification of donor/non-donor or DAMT, the amount of donation for a donor.

Now that our datasets are split, let's examine the feature set of the training dataset.  Listed below is a table of the features encompassed in the dataset and the description for what it represents.  We notice a few groups of features.  We have a few location features, potential donor demographics features, income information about the potential donor's neighborhood features, and past donation and promotion history if applicable. 

Feature | Description
-----------------------------|-------------------------------------------------
ID number | Number used as an ID
REG1 | Potential donor located in region 1
REG2 | Potential donor located in region 2
REG3 | Potential donor located in region 3
REG4 | Potential donor located in region 4
HOME | 1 = homeowner, 0 = not a homeowner
CHLD | Number of children
HINC | Household income (7 categories)
GENF | Gender (0 = Male, 1 = Female)
WRAT | Wealth Rating (0 lowest - 9 highest)
AVHV | Average Home Value in potential donor's neighborhood in $ thousands
INCM | Median Family Income in potential donor's neighborhood in $ thousands
INCA | Average Family Income in potential donor's neighborhood in $ thousands
PLOW | Percent categorized as “low income” in potential donor's neighborhood
NPRO | Lifetime number of promotions received to date
TGIF | Dollar amount of lifetime gifts to date
LGIF | Dollar amount of largest gift to date
RGIF | Dollar amount of most recent gift
TDON | Number of months since last donation
TLAG | Number of months between first and second gift
AGIF | Average dollar amount of gifts to date
DONR | Classification Response Variable (1 = Donor, 0 = Non-donor)
DAMT | Prediction Response Variable (Donation Amount in $).


After our qualitative observations of the feature set, let's plot a correlation matrix to see the correlation of the features and their relationship with our response variables. 

```{r include=FALSE}
#remove ID 
data.training<- data.train[, 2:24]

#install.packages("corrgram")
library(corrgram)
corrgram(data.training, order=NULL, lower.panel=panel.shade,
  upper.panel=NULL, text.panel=panel.txt,
  main="Training Data - Correlogram")

```

In this graphic, the heavily shaded blue boxes are indicating high positive correlations and the heavily shaded red boxes are a representation of a negative correlation.  This correlogram shows us a few things we might except, for example if someone donates, their donation amount is positive.  Additionaly, the regions have a negative correlation with the other regions since each potential donor can only be located in one region.  The features about income are related to donor and donor amount as well.  The heaviest shaded areas for donor and donor amount appear to be the features indicating home ownership and the number of children in the household. 

Let's dig deeper into some of the features that popped from the correlgram.  Plotting the donation amount for the factors of home ownership (home = 0 for non-homeowner, and home = 1 for home owner) reveals the following:

```{r include=FALSE}

library(lattice)
densityplot(~data.training$damt|as.factor(data.training$home), 
  	main="Density Plot by Home Ownership",
   xlab="Donation Amount")

```

Visualizing the donation amount by the factors of home ownership is pretty revealing.  Here we see that those that are home owners have significantly more observations with donations.  An additional benefit to visualizing by donation amount is that we can also see the binary classification of donor relationship.  Since someone who does not donate would have a donation amount = 0, we can observe the amount of observations with 0 donation amount to see the amount of non-donors.  Clearly those that own homes have significantly more donors and more donations.  This is powerful evidence that this feature should be included in our modeling exercise. 

Additionally, the wealth rating also was positively correlated with donation amount so let's plot that against donation amount as well.

```{r include=FALSE}

library(lattice)
densityplot(~data.training$damt|as.factor(data.training$wrat), 
  	main="Density Plot by Wealth Rating",
   xlab="Donation Amount",
   layout=c(4,3))

```

Wealth rating uses 10 levels, 0-9 with 0 being the lowest level of wealth of 9 the highest.   Observing the graphic we notice that the lower levels of wealth have more non-donors, while the highest level of income have significantly more donors and donation amounts.  One thing to note is that the lowest levels of 0 and 1 seem to have a similar distribution, while the rest of the levels have a fairly similar distribution.  We should consider making an additional variable that lists wealth as low (0, 1) or high (2-9).

There are quite a few income-related features similiar to wealth rating, which begs the question of if we need to include all of them in the models we develop.  If they are highly correlated, they are providing redundant information and will ultimately hurt certain modeling approaches such as linear regression.

```{r include=FALSE}

splom(data.training[c(9, 10, 11, 12, 13, 22)], main = 'Income Features', pscales = 0, axis.line.tck = 0)

```

Examining the top line of the scatterplot matrix we can see how each of the income features impact the donation amount.  It's fairly obvious from this graphic that average home value in neighborhood, average income, and median income in potential donor's neighborhood have a very similar relationship with the donation amount.  The thickness of the amount of 0 donations in each of these feature's grid also tells us that there are a similar amount of non-donors in each of these features as well.  Interestingly, the wealth rating appears to have a slightly different relationship with donation amount telling us that some combination of wealth and income features would be the predictive.  

Historically, income or currency related features tend to have a positive skew, so let's examine them singularly to see if we need to transform them to make them behave more "normal.  Below is a density plot of median income.

```{r include=FALSE}

densityplot(~data.training$incm, 
  	main="Density Plot", 
  	xlab="Median Income")

```


Clearly, we are seeing some skew with this distribution as evidenced by the fat tails on the far left.  Typically a log transformation can take of this type of skewness.  Let's transform each of the income features with a log to verify that we have a created a more normal distribution which will be beneficial for our modeling.

```{r include=FALSE}

densityplot(~log(data.training$incm), 
  	main="log(incm)", 
  	xlab="Median Income")

densityplot(~log(data.training$inca), 
  	main="log(inca)", 
  	xlab="Average Income")

densityplot(~log(data.training$avhv), 
  	main="log(avhv)", 
  	xlab="Average Home Value")

densityplot(~log(data.training$plow), 
  	main="log(plow)", 
  	xlab="Percentage of Low Income")

```

The transformations appear to be creating much better distributions for use in our models, as normal distributions behave better in regression modeling.  

Much like the income grouping of features there are several features in regards to past donation histroy.  Let's examine them together to see if they too are possibly providing redundant information.  

```{r include=FALSE}

splom(data.training[c(15, 16, 17, 18, 19, 20, 22)], main = 'Donation Features', pscales = 0, axis.line.tck = 0)

```

It appears that the features related to donation amounts have similar charecterisitcs and the feature related to timing of donations are similar.  Let's split them up and look at them seperately.  

```{r include=FALSE}

splom(data.training[c(15, 16, 17, 20, 22)], main = 'Dollar Donation Features', pscales = 0, axis.line.tck = 0)
splom(data.training[c(18, 19, 22)], main = 'Timing Donation Features', pscales = 0, axis.line.tck = 0)

```

Seperating out the timing and dollar donations features we can see two different relationships forming with donation amount.  It would be reasonable to assume that we would like to have some aspect of timing and amount in our modeling; and since the features in these two buckets are very similar we most likely do not need to include all of them.

Let's examing the dollar donation features to see if we need to transform them like we did with the other currency related features.

```{r include=FALSE}

densityplot(~data.training$tgif, 
  	main="Density Plot", 
  	xlab="Donation amount to Date")

```

As we saw before with the currency based features, we notice a large skew and a fat tail on the right of the distribution.  Let's see if we can acheive the same dynamic we did before with a log transformation.

```{r include=FALSE}

densityplot(~log(data.training$tgif), 
  	main="log(tgif)", 
  	xlab="Lifetime Donations")

densityplot(~log(data.training$lgif), 
  	main="log(lgif)", 
  	xlab="Largest Donation")

densityplot(~log(data.training$rgif), 
  	main="log(rgif)", 
  	xlab="Recent Donation")

densityplot(~log(data.training$agif), 
  	main="log(agif)", 
  	xlab="Average Donation")

```

Again, it looks like this log transformation will be beneficial as it's creating a more normal distibution.  

The last feature we will examine is the number of children in household.  If we remember from our correlogram, this feature was the most negatively correlated with donation amount, telling us that there appears to be a relationship between the two that could be beneficial in our prediction. 

```{r include=FALSE}

densityplot(~data.training$damt|as.factor(data.training$chld), 
  	main="Density Plot by Number of Children",
   xlab="Donation Amount",
   layout=c(2,3))

```

It's clear from this graphic that there is an inverse relationship with donation amount and number of children in household, just as the correlation suggested.  The density plots for potential donors with 0 and 5 kids seem to be almost exactly opposite.  This is providing very strong evidence that the number of children in the household will be very useful in predicting not only a donor, but the amount of donation. 

Summarizng the learnings from our EDA are as follows:

* Home-owners historically have donated more than non-home owners
* Wealth rating has a strong relationship with donation amount
* Wealth rating appears to be broken into two groups, low (0, 1) and high (2-9)
* All income features have a similar relationship with donation amount
* Transforming income features with logs makes them more normal due to their positive skewness
* Past donation behavior is grouped into two buckets, dollar amount and timing
* Dollar donations features benefit from log transformations
* Number of children in the household is negatively correlated with donation amount

### Feature Transformation

Before we start modeling, we need to incorporate the learnings from the EDA we just performed.  First we will created a binary feature for wealth, indicating high or low income.

```{r include=FALSE}

wrat_cat<- as.numeric(ifelse(charity$wrat <= 1, 0, 1))
charity2<- cbind(charity, wrat_cat)
plot(x = charity2$wrat_cat, charity2$wrat)

```

We are creating this indicator variable because we saw that potential donors with wealth rating of 0 and 1 behaved similar and those that had ratings of 2+ were also similar.  This added feature is created a slightly different interpretation of wealth which might be more predictive.  

Next we need to use a log transformation on all of the income features and dollar donations.

```{r include=FALSE}

charity.f<- charity2
charity.f$incm <- log(charity2$incm)
charity.f$inca <- log(charity2$inca)
charity.f$avhv <- log(charity2$avhv)
charity.f$tgif <- log(charity2$tgif)
charity.f$lgif <- log(charity2$lgif)
charity.f$rgif <- log(charity2$rgif)
charity.f$agif <- log(charity2$agif)

```

Now that we have our features transformed, we can create our final training, validation, and testing datasets to be used for modeling.  Through this process we will seperate our predictor variables from our response variable.  Additionally, we will scale the features so that they are providing an equal weight to the model predictions. 

```{r include=FALSE}

data.train <- charity.f[charity.f$part=="train",]
x.train <- data.train[,c(2:21,25)]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995

data.valid <- charity.f[charity.f$part=="valid",]
x.valid <- data.valid[,c(2:21,25)]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999

data.test <- charity.f[charity.f$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,c(2:21, 25)]


x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
#apply(x.train.std, 2, mean) # check zero mean
#apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)

```

## Classification Model

The first aspect of the analysis for the charitable organizations is to predict whether or not a potential donor will in fact donate.  To do, we will create a binary classification of donor/non-donor using a variety of classification techniques.  The techniques that we will use in the analysis are as follows:

* Linear Discriminant Analysis
* Quadractic Discriminant Analysis
* Logistic Regression
* k-NN Classification
* Support Vector Machine Classification
* Random Forest Classification
* Gradient Boosted Decision Trees

In order to evaluate which model is optimal we will calculate the maximum profit produce given the classification model.  The maximum profit is defined as the value for i for which the following formula is maximized:

$$\sum_{i=1}^n 14.5*Pr - 2$$

The average donation from someone contacted with the direct marketing is \$14.50, and the cost of each mailing is \$2.00.  As we iterate through our potential donors, the function will reach a maximum at some value of i, or some circulation.  To optimize the function we need to do a good job at predicting responders, and generating high posterior probabilities.  These high probabilities will result in more benefit (14.5 \* 0.99 vs. 14.5 \* 0.50) as we will only mail the potential donors with a high probability to respond and we won't "waste" circulation on those that are not going to donate.

### Linear Discriminant Analysis (LDA)

The first classification model we will attempt to predict the probabilities of potential donors actually being a donor is linear discriminant analysis.  LDA models the distribution of predictors seperatly in each of the response classes and then uses bayes thereom in order to create probabilities (Section 4.4, James, Witten, Hastie, Tibshirani).  As it's name would suggest this model assumes a linear relationship.  

The general form that we will follow for this classification models is to learn the model on the training dataset, and then once we have our model parameters we will predict the validation test set.  The model with the best validation set performance will be used as our final model to predict our test set.  

```{r include=FALSE}

library(MASS)

#learning an LDA model with leave one out cross validation
model.lda1 <- lda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif + wrat_cat, 
                  data.train.std.c, cv = TRUE) 

#creating probability esimates for the validation dataset
post.valid.lda1 <- predict(model.lda1, data.valid.std.c)$posterior[,2] # n.valid.c post probs

#install.packages("ROCR")
library(ROCR)
pred <- prediction(post.valid.lda1,data.valid.std.c$donr)
perf <- performance(pred,"tpr","fpr")
plot(perf, main ="ROC Curve for LDA")
```

After learning the model and generating predictions we plot these predictions as a function of true positives and false positives to see how well the model is predicting.  A perfect prediction would look like a straight line up from the y-axis and then a 90 degree turn to the right once it reached the value one.  This is because we would predict every donor correct first before we saw one wrong prediction.  This model has a fairly enticing ROC curve, it's by no means perfect but in general it's doing a fairly decent job.  

Now let's calculate what the maximum profit is for this model given the function above for maximum profit. 

```{r include=FALSE}

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.lda1 <- cumsum(14.5*c.valid[order(post.valid.lda1, decreasing=T)]-2)
plot(profit.lda1, main="Profit by Circulation") 
n.mail.valid <- which.max(profit.lda1) 
c(n.mail.valid, max(profit.lda1)) 

```

```{r include=FALSE}
cutoff.lda1 <- sort(post.valid.lda1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.lda1 <- ifelse(post.valid.lda1>cutoff.lda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda1, c.valid) # classification table
#               c.valid
#chat.valid.lda1   0   1
#              0 675  14
#              1 344 985
# check n.mail.valid = 344+985 = 1329
# check profit = 14.5*985-2*1329 = 11624.5



```

```{r include=FALSE}

pred <- prediction( ROCR.simple$predictions, ROCR.simple$labels )
pred2 <- prediction(abs(ROCR.simple$predictions + 
                        rnorm(length(ROCR.simple$predictions), 0, 0.1)), 
        ROCR.simple$labels)
perf <- performance( pred, "tpr", "fpr" )
perf2 <- performance(pred2, "tpr", "fpr")
plot( perf, colorize = TRUE)
plot(perf2, add = TRUE, colorize = TRUE)

```
# 2) Develop a prediction model to predict donation amounts for donors - the data
# for this will consist of the records for donors only.




##### CLASSIFICATION MODELING ######

# linear discriminant analysis

library(MASS)

model.lda1 <- lda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                  data.train.std.c) # include additional terms on the fly using I()

# Note: strictly speaking, LDA should not be used with qualitative predictors,
# but in practice it often is if the goal is simply to find a good predictive model

post.valid.lda1 <- predict(model.lda1, data.valid.std.c)$posterior[,2] # n.valid.c post probs

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2

profit.lda1 <- cumsum(14.5*c.valid[order(post.valid.lda1, decreasing=T)]-2)
plot(profit.lda1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.lda1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.lda1)) # report number of mailings and maximum profit
# 1329.0 11624.5

cutoff.lda1 <- sort(post.valid.lda1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.lda1 <- ifelse(post.valid.lda1>cutoff.lda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda1, c.valid) # classification table
#               c.valid
#chat.valid.lda1   0   1
#              0 675  14
#              1 344 985
# check n.mail.valid = 344+985 = 1329
# check profit = 14.5*985-2*1329 = 11624.5

# logistic regression

model.log1 <- glm(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                  data.train.std.c, family=binomial("logit"))

post.valid.log1 <- predict(model.log1, data.valid.std.c, type="response") # n.valid post probs

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2

profit.log1 <- cumsum(14.5*c.valid[order(post.valid.log1, decreasing=T)]-2)
plot(profit.log1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.log1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.log1)) # report number of mailings and maximum profit
# 1291.0 11642.5

cutoff.log1 <- sort(post.valid.log1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.log1 <- ifelse(post.valid.log1>cutoff.log1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.log1, c.valid) # classification table
#               c.valid
#chat.valid.log1   0   1
#              0 709  18
#              1 310 981
# check n.mail.valid = 310+981 = 1291
# check profit = 14.5*981-2*1291 = 11642.5

# Results

# n.mail Profit  Model
# 1329   11624.5 LDA1
# 1291   11642.5 Log1

# select model.log1 since it has maximum profit in the validation sample

post.test <- predict(model.log1, data.test.std, type="response") # post probs for test data

# Oversampling adjustment for calculating number of mailings for test set

n.mail.valid <- which.max(profit.log1)
tr.rate <- .1 # typical response rate is .1
vr.rate <- .5 # whereas validation response rate is .5
adj.test.1 <- (n.mail.valid/n.valid.c)/(vr.rate/tr.rate) # adjustment for mail yes
adj.test.0 <- ((n.valid.c-n.mail.valid)/n.valid.c)/((1-vr.rate)/(1-tr.rate)) # adjustment for mail no
adj.test <- adj.test.1/(adj.test.1+adj.test.0) # scale into a proportion
n.mail.test <- round(n.test*adj.test, 0) # calculate number of mailings for test set

cutoff.test <- sort(post.test, decreasing=T)[n.mail.test+1] # set cutoff based on n.mail.test
chat.test <- ifelse(post.test>cutoff.test, 1, 0) # mail to everyone above the cutoff
table(chat.test)
#    0    1 
# 1676  331
# based on this model we'll mail to the 331 highest posterior probabilities

# See below for saving chat.test into a file for submission



##### PREDICTION MODELING ######

# Least squares regression

model.ls1 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls1 <- predict(model.ls1, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls1)^2) # mean prediction error
# 1.867523
sd((y.valid - pred.valid.ls1)^2)/sqrt(n.valid.y) # std error
# 0.1696615

# drop wrat for illustrative purposes
model.ls2 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls2 <- predict(model.ls2, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls2)^2) # mean prediction error
# 1.867433
sd((y.valid - pred.valid.ls2)^2)/sqrt(n.valid.y) # std error
# 0.1696498

# Results

# MPE  Model
# 1.867523 LS1
# 1.867433 LS2

# select model.ls2 since it has minimum mean prediction error in the validation sample

yhat.test <- predict(model.ls2, newdata = data.test.std) # test predictions




# FINAL RESULTS

# Save final results for both classification and regression

length(chat.test) # check length = 2007
length(yhat.test) # check length = 2007
chat.test[1:10] # check this consists of 0s and 1s
yhat.test[1:10] # check this consists of plausible predictions of damt

ip <- data.frame(chat=chat.test, yhat=yhat.test) # data frame with two variables: chat and yhat
write.csv(ip, file="ABC.csv", row.names=FALSE) # use your initials for the file name

# submit the csv file in Angel for evaluation based on actual test donr and damt values
