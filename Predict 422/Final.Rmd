---
title: "PRED422_Final"
author: "Reed Thunstrom"
subtitle: Predict 422 | Section 58
output: word_document
---

## Introduction

The purpose of this report is to aid a charitable organization with their direct marketing efforts.  The charitable organiziation has a list of potential donors with the history of the past donations, this will be the dataset used for the analysis.  The data set consists of 21 features or attributes about the potential donors as well as if the donor has donated in the past, and if so how much.  We will separate the file into a training dataset used to learn the models and a validation dataset which will be used to assess the predictive accuracy of each model.  This analysis will consists of two parts: the first is a classification model used to predict if a target of the direct marketing will donate and a regression model to predict the amount of donation if they were to donate.  These two components will be invaluable to the charity organiziation as they will be able to optimize their mailing for maximum net donations after considering the cost of the mailing.  

Each donor, on average, donates \$14.50 and each piece of direct marketing costs \$2.00. We will use these two metrics in our analysis to define an optimal mailing which will produce the maximum net donations for the charitable organization.  Now that we have describe the goal of the analysis, let's start to expolore the data that is available. 

## Exploratory Data Analysis (EDA)

First let's load in our data and take a look at the first few rows of data. 

```{r setup, include=FALSE}

charity <- read.csv('/Users/z013nx1/Documents/charity.csv') # load the "charity.csv" file
head(charity)

```

We notice that we have 24 columns, 1 ID, 20 features, 1 classification dependent variable (DONR), 1 regression dependent variable(DAMT), and 1 column seperating the file into our training, testing, and validation datasets.  Let's split up of file into the three modeling subsets and start to explore the feature set.

```{r include=FALSE}
data.train <- charity[charity$part=="train",]
data.valid <- charity[charity$part=="valid",]
data.test <- charity[charity$part=="test",]
length(data.train[,1])
length(data.valid[,1])
length(data.test[,1])
```

The training dataset has 3,984 observations and we will validate our models on the validation set which consists of 2,018 observations.  The testing set that we will deploy our final models on has 2,007 observations and we do not have any values for DONR, the binary classification of donor/non-donor or DAMT, the amount of donation for a donor.

Now that our datasets are split, let's examine the feature set of the training dataset.  Listed below is a table of the features encompassed in the dataset and the description for what it represents.  We notice a few groups of features.  We have a few location features, potential donor demographics features, income information about the potential donor's neighborhood features, and past donation and promotion history if applicable. 

Feature | Description
-----------------------------|-------------------------------------------------
ID number | Number used as an ID
REG1 | Potential donor located in region 1
REG2 | Potential donor located in region 2
REG3 | Potential donor located in region 3
REG4 | Potential donor located in region 4
HOME | 1 = homeowner, 0 = not a homeowner
CHLD | Number of children
HINC | Household income (7 categories)
GENF | Gender (0 = Male, 1 = Female)
WRAT | Wealth Rating (0 lowest - 9 highest)
AVHV | Average Home Value in potential donor's neighborhood in $ thousands
INCM | Median Family Income in potential donor's neighborhood in $ thousands
INCA | Average Family Income in potential donor's neighborhood in $ thousands
PLOW | Percent categorized as “low income” in potential donor's neighborhood
NPRO | Lifetime number of promotions received to date
TGIF | Dollar amount of lifetime gifts to date
LGIF | Dollar amount of largest gift to date
RGIF | Dollar amount of most recent gift
TDON | Number of months since last donation
TLAG | Number of months between first and second gift
AGIF | Average dollar amount of gifts to date
DONR | Classification Response Variable (1 = Donor, 0 = Non-donor)
DAMT | Prediction Response Variable (Donation Amount in $).


After our qualitative observations of the feature set, let's plot a correlation matrix to see the correlation of the features and their relationship with our response variables. 

```{r include=FALSE}
#remove ID 
data.training<- data.train[, 2:24]

#install.packages("corrgram")
library(corrgram)
corrgram(data.training, order=NULL, lower.panel=panel.shade,
  upper.panel=NULL, text.panel=panel.txt,
  main="Training Data - Correlogram")

```

In this graphic, the heavily shaded blue boxes are indicating high positive correlations and the heavily shaded red boxes are a representation of a negative correlation.  This correlogram shows us a few things we might except, for example if someone donates, their donation amount is positive.  Additionaly, the regions have a negative correlation with the other regions since each potential donor can only be located in one region.  The features about income are related to donor and donor amount as well.  The heaviest shaded areas for donor and donor amount appear to be the features indicating home ownership and the number of children in the household. 

Let's dig deeper into some of the features that popped from the correlgram.  Plotting the donation amount for the factors of home ownership (home = 0 for non-homeowner, and home = 1 for home owner) reveals the following:

```{r include=FALSE}

library(lattice)
densityplot(~data.training$damt|as.factor(data.training$home), 
  	main="Density Plot by Home Ownership",
   xlab="Donation Amount")

```

Visualizing the donation amount by the factors of home ownership is pretty revealing.  Here we see that those that are home owners have significantly more observations with donations.  An additional benefit to visualizing by donation amount is that we can also see the binary classification of donor relationship.  Since someone who does not donate would have a donation amount = 0, we can observe the amount of observations with 0 donation amount to see the amount of non-donors.  Clearly those that own homes have significantly more donors and more donations.  This is powerful evidence that this feature should be included in our modeling exercise. 

Additionally, the wealth rating also was positively correlated with donation amount so let's plot that against donation amount as well.

```{r include=FALSE}

library(lattice)
densityplot(~data.training$damt|as.factor(data.training$wrat), 
  	main="Density Plot by Wealth Rating",
   xlab="Donation Amount",
   layout=c(4,3))

```

Wealth rating uses 10 levels, 0-9 with 0 being the lowest level of wealth of 9 the highest.   Observing the graphic we notice that the lower levels of wealth have more non-donors, while the highest level of income have significantly more donors and donation amounts.  One thing to note is that the lowest levels of 0 and 1 seem to have a similar distribution, while the rest of the levels have a fairly similar distribution.  We should consider making an additional variable that lists wealth as low (0, 1) or high (2-9).

There are quite a few income-related features similiar to wealth rating, which begs the question of if we need to include all of them in the models we develop.  If they are highly correlated, they are providing redundant information and will ultimately hurt certain modeling approaches such as linear regression.

```{r include=FALSE}

splom(data.training[c(9, 10, 11, 12, 13, 22)], main = 'Income Features', pscales = 0, axis.line.tck = 0)

```

Examining the top line of the scatterplot matrix we can see how each of the income features impact the donation amount.  It's fairly obvious from this graphic that average home value in neighborhood, average income, and median income in potential donor's neighborhood have a very similar relationship with the donation amount.  The thickness of the amount of 0 donations in each of these feature's grid also tells us that there are a similar amount of non-donors in each of these features as well.  Interestingly, the wealth rating appears to have a slightly different relationship with donation amount telling us that some combination of wealth and income features would be the predictive.  

Historically, income or currency related features tend to have a positive skew, so let's examine them singularly to see if we need to transform them to make them behave more "normal.  Below is a density plot of median income.

```{r include=FALSE}

densityplot(~data.training$incm, 
  	main="Density Plot", 
  	xlab="Median Income")

```


Clearly, we are seeing some skew with this distribution as evidenced by the fat tails on the far left.  Typically a log transformation can take of this type of skewness.  Let's transform each of the income features with a log to verify that we have a created a more normal distribution which will be beneficial for our modeling.

```{r include=FALSE}

densityplot(~log(data.training$incm), 
  	main="log(incm)", 
  	xlab="Median Income")

densityplot(~log(data.training$inca), 
  	main="log(inca)", 
  	xlab="Average Income")

densityplot(~log(data.training$avhv), 
  	main="log(avhv)", 
  	xlab="Average Home Value")

densityplot(~log(data.training$plow), 
  	main="log(plow)", 
  	xlab="Percentage of Low Income")

```

The transformations appear to be creating much better distributions for use in our models, as normal distributions behave better in regression modeling.  

Much like the income grouping of features there are several features in regards to past donation histroy.  Let's examine them together to see if they too are possibly providing redundant information.  

```{r include=FALSE}

splom(data.training[c(15, 16, 17, 18, 19, 20, 22)], main = 'Donation Features', pscales = 0, axis.line.tck = 0)

```

It appears that the features related to donation amounts have similar charecterisitcs and the feature related to timing of donations are similar.  Let's split them up and look at them seperately.  

```{r include=FALSE}

splom(data.training[c(15, 16, 17, 20, 22)], main = 'Dollar Donation Features', pscales = 0, axis.line.tck = 0)
splom(data.training[c(18, 19, 22)], main = 'Timing Donation Features', pscales = 0, axis.line.tck = 0)

```

Seperating out the timing and dollar donations features we can see two different relationships forming with donation amount.  It would be reasonable to assume that we would like to have some aspect of timing and amount in our modeling; and since the features in these two buckets are very similar we most likely do not need to include all of them.

Let's examing the dollar donation features to see if we need to transform them like we did with the other currency related features.

```{r include=FALSE}

densityplot(~data.training$tgif, 
  	main="Density Plot", 
  	xlab="Donation amount to Date")

```

As we saw before with the currency based features, we notice a large skew and a fat tail on the right of the distribution.  Let's see if we can acheive the same dynamic we did before with a log transformation.

```{r include=FALSE}

densityplot(~log(data.training$tgif), 
  	main="log(tgif)", 
  	xlab="Lifetime Donations")

densityplot(~log(data.training$lgif), 
  	main="log(lgif)", 
  	xlab="Largest Donation")

densityplot(~log(data.training$rgif), 
  	main="log(rgif)", 
  	xlab="Recent Donation")

densityplot(~log(data.training$agif), 
  	main="log(agif)", 
  	xlab="Average Donation")

```

Again, it looks like this log transformation will be beneficial as it's creating a more normal distibution.  

The last feature we will examine is the number of children in household.  If we remember from our correlogram, this feature was the most negatively correlated with donation amount, telling us that there appears to be a relationship between the two that could be beneficial in our prediction. 

```{r include=FALSE}

densityplot(~data.training$damt|as.factor(data.training$chld), 
  	main="Density Plot by Number of Children",
   xlab="Donation Amount",
   layout=c(2,3))

```

It's clear from this graphic that there is an inverse relationship with donation amount and number of children in household, just as the correlation suggested.  The density plots for potential donors with 0 and 5 kids seem to be almost exactly opposite.  This is providing very strong evidence that the number of children in the household will be very useful in predicting not only a donor, but the amount of donation. 

Summarizng the learnings from our EDA are as follows:

* Home-owners historically have donated more than non-home owners
* Wealth rating has a strong relationship with donation amount
* Wealth rating appears to be broken into two groups, low (0, 1) and high (2-9)
* All income features have a similar relationship with donation amount
* Transforming income features with logs makes them more normal due to their positive skewness
* Past donation behavior is grouped into two buckets, dollar amount and timing
* Dollar donations features benefit from log transformations
* Number of children in the household is negatively correlated with donation amount

### Feature Transformation

Before we start modeling, we need to incorporate the learnings from the EDA we just performed.  First we will created a binary feature for wealth, indicating high or low income.

```{r include=FALSE}

wrat_cat<- as.numeric(ifelse(charity$wrat <= 1, 0, 1))
charity2<- cbind(charity, wrat_cat)
plot(x = charity2$wrat_cat, charity2$wrat)

```

We are creating this indicator variable because we saw that potential donors with wealth rating of 0 and 1 behaved similar and those that had ratings of 2+ were also similar.  This added feature is created a slightly different interpretation of wealth which might be more predictive.  

Next we need to use a log transformation on all of the income features and dollar donations.

```{r include=FALSE}

charity.f<- charity2
charity.f$incm <- log(charity2$incm)
charity.f$inca <- log(charity2$inca)
charity.f$avhv <- log(charity2$avhv)
charity.f$tgif <- log(charity2$tgif)
charity.f$lgif <- log(charity2$lgif)
charity.f$rgif <- log(charity2$rgif)
charity.f$agif <- log(charity2$agif)

```

Now that we have our features transformed, we can create our final training, validation, and testing datasets to be used for modeling.  Through this process we will seperate our predictor variables from our response variable.  Additionally, we will scale the features so that they are providing an equal weight to the model predictions. 

```{r include=FALSE}

data.train <- charity.f[charity.f$part=="train",]
x.train <- data.train[,c(2:21,25)]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995

data.valid <- charity.f[charity.f$part=="valid",]
x.valid <- data.valid[,c(2:21,25)]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999

data.test <- charity.f[charity.f$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,c(2:21, 25)]


x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
#apply(x.train.std, 2, mean) # check zero mean
#apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)

```

## Classification Model

The first aspect of the analysis for the charitable organizations is to predict whether or not a potential donor will in fact donate.  To do, we will create a binary classification of donor/non-donor using a variety of classification techniques.  The techniques that we will use in the analysis are as follows:

* Linear Discriminant Analysis
* Quadractic Discriminant Analysis
* Logistic Regression
* k-NN Classification
* Random Forest Classification
* Gradient Boosted Decision Trees
* Support Vector Machine Classification

In order to evaluate which model is optimal we will calculate the maximum profit produce given the classification model.  The maximum profit is defined as the value for i for which the following formula is maximized:

$$14.5*Responders - 2*Total Mail Circulation$$

The average donation from someone contacted with the direct marketing is \$14.50, and the cost of each mailing is \$2.00.  As we iterate through our potential donors, the function will reach a maximum at some value of i, or some circulation.  To optimize the function we need to do a good job at predicting responders, and generating high posterior probabilities.  The more responders when we get as a % of out total mail circulation will produce higher profits, so it's vital to get the highest response rate as possible.

### Linear Discriminant Analysis (LDA)

The first classification model we will attempt to predict the probabilities of potential donors actually being a donor is linear discriminant analysis.  LDA models the distribution of predictors seperatly in each of the response classes and then uses bayes thereom in order to create probabilities (Section 4.4, James, Witten, Hastie, Tibshirani).  As it's name would suggest this model assumes a linear relationship.  

The general form that we will follow for this classification models is to learn the model on the training dataset, and then once we have our model parameters we will predict the validation test set.  The model with the best validation set performance will be used as our final model to predict our test set.  

```{r include=FALSE}

library(MASS)

#learning an LDA model with leave one out cross validation
model.lda1 <- lda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif + wrat_cat, 
                  data.train.std.c, cv = TRUE) 

#creating probability esimates for the validation dataset
post.valid.lda1 <- predict(model.lda1, data.valid.std.c)$posterior[,2] # n.valid.c post probs

#install.packages("ROCR")
library(ROCR)
pred <- prediction(post.valid.lda1,data.valid.std.c$donr)
perf <- performance(pred,"tpr","fpr")
plot(perf, main ="ROC Curve for LDA")
```

After learning the model and generating predictions we plot these predictions as a function of true positives and false positives to see how well the model is predicting.  A perfect prediction would look like a straight line up from the y-axis and then a 90 degree turn to the right once it reached the value one.  This is because we would predict every donor correct first before we saw one wrong prediction.  This model has a fairly enticing ROC curve, it's by no means perfect but in general it's doing a fairly decent job.  

Now let's calculate what the maximum profit is for this model given the function above for maximum profit. 

```{r include=FALSE}

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.lda1 <- cumsum(14.5*c.valid[order(post.valid.lda1, decreasing=T)]-2)
plot(profit.lda1, main="Profit by Circulation") 
n.mail.valid <- which.max(profit.lda1) 
c(n.mail.valid, max(profit.lda1)) 

```

The maximum profit for the LDA model is $11,355.50 acheived by mailing the first 1391 potential donors of the total 2018 in the validation dataset. The graph does a great job at visualizing the maximum profit.  The profit is steadily rising until we hit the 1,391st potential donor, then it starts to drop telling us that any mailing past that point is a waste.

The 1,391st observation is the cutoff and we will mail to everyone up to that value.  As a result, we will have a certain number of donor responses and non-responses.  Those that respond will produce 14.50 of donations and then we will subtract the total mailing cost from everyone that was mailed to arrive at profit. 

```{r include=FALSE}
cutoff.lda1 <- sort(post.valid.lda1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.lda1 <- ifelse(post.valid.lda1>cutoff.lda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda1, c.valid) # classification table
```

Observing the chart we see that we mail everyone that gets a predicted label of 1 (416 + 975 = 1391) and we get a donation from 975.  As a result the profit is:

$$14.5* 975 (Responders) - 2* 1391 (Total Mail Circulation) = 11,355.50 (Profit)$$

## Quadractic Discriminant Analysis (QDA)

The next classification model we will explore is a quadractic discriminant analysis.  Much like the name would suggest, it's very similar to the LDA.  The difference is that QDA can generate a non-linear decision boundary (Section 4.4.4, James, Witten, Hastie, Tibshirani)

```{r include=FALSE}

model.qda1 <- qda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif + wrat_cat, 
                  data.train.std.c, cv = TRUE) 

#creating probability esimates for the validation dataset
post.valid.qda1 <- predict(model.qda1, data.valid.std.c)$posterior[,2] # n.valid.c post probs

pred <- prediction(post.valid.qda1,data.valid.std.c$donr)
perf <- performance(pred,"tpr","fpr")
plot(perf, main ="ROC Curve for QDA")

```

After learning the QDA model, predicting the probabilities, and visualizing these in our ROC curve we notice very similar performance as the LDA model.  

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.qda1 <- cumsum(14.5*c.valid[order(post.valid.qda1, decreasing=T)]-2)
plot(profit.qda1, main="Profit by Circulation") 
n.mail.valid.qda <- which.max(profit.qda1) 
c(n.mail.valid.qda, max(profit.qda1)) 
```

The optimal circulation from the QDA model is slightly smaller than that of the LDA, however the profit is smaller as well.  So even though this model "saves" a little circulation cost, it does a worse job predicting responses so we don't get as many donations in our profit function.

```{r include=FALSE}
cutoff.qda1 <- sort(post.valid.qda1, decreasing=T)[n.mail.valid.qda+1] # set cutoff based on n.mail.valid
chat.valid.qda1 <- ifelse(post.valid.qda1>cutoff.qda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.qda1, c.valid) # classification table
```

As we pointed out above, we are only mailing 1,285 potential donors, but we are predicting 25 less donors than the LDA model (950 vs. 975) and as a result are losing out on \$362.50 of donations, but only saving \$212 of mailing cost ((1391 - 1285) * 2).  We are starting to see that prediction of respondents is more important than lower circulation. 

$$14.5* 950 (Responders) - 2* 1285 (Total Mail Circulation) = 11,205 (Profit)$$

## Logistic Regression

Logistic regression is one of the most common and widely used techniques for classification.  It's an expansion of linear regression, but the function is constrained to predictions (which are probabilities) between 0 and 1 through the sigmoid function.  (Section 4.3, James, Witten, Hastie, Tibshirani)  Logistic regression models conditional distribution of Y, which is slightly different than LDA and QDA which model each reponse class seperately.


```{r include=FALSE}

model.lr <- glm(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + avhv + incm + inca + plow + npro + tgif +
               lgif + rgif + tdon + tlag + agif + wrat_cat, data = data.train.std.c, family = binomial) 

#creating probability esimates for the validation dataset
post.valid.lr=predict(model.lr,data.valid.std.c,type="response")

pred <- prediction(post.valid.lr,data.valid.std.c$donr)
perf <- performance(pred,"tpr","fpr")
plot(perf, main ="ROC Curve for Logistic Regression")

```

The model appears to producing very similar results to the QDA and LDA models.  When the distribution of the are assumed to be normal, this is common (Section 4.4, James, Witten, Hastie, Tibshirani)

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.lr <- cumsum(14.5*c.valid[order(post.valid.lr, decreasing=T)]-2)
plot(profit.lr, main="Profit by Circulation") 
n.mail.valid.lr <- which.max(profit.lr) 
c(n.mail.valid.lr, max(profit.lr)) 
```

The logistic model produces a maximum profit slightly higher than the other models at $11,390.50 and a circulation of 1446.  This is telling us that this model does a better job at recall the positive responses, which is the objective of the model and the way in which it produces higher profit since the benefit outweighs the cost (14.50 > 2.00)

```{r include=FALSE}
cutoff.lr <- sort(post.valid.lr, decreasing=T)[n.mail.valid.lr+1] # set cutoff based on n.mail.valid
chat.valid.lr <- ifelse(post.valid.lr>cutoff.lr, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lr, c.valid) # classification table
```

As we suspected, this model generates more responders than either of the models at 985.  It does achieve this at quite a bit more circulation, but the benefit exceed the costs.

$$14.5* 985 (Responders) - 2* 1446(Total Mail Circulation) = 11,390.50 (Profit)$$

### Logistic Regression | L2 Regularization

Remember back to the EDA that we performed earlier, we noticed that there were quite a few correlated features and qualitative grouping of features: location, income, and donation history.  Parametric models with many colinear variables can suffer from multicollinearity which produces unreliable coefficient estimates.  To combat this type of colinear feature selection, we can apply some regularization or penalty to the feature set.  The penalty term works much the same way that our maximum profit function works.  The benefit from adding an additional feature needs to outweigh the cost for adding it.  There are two popular regularization terms that we will evaluate.  The first is the L2 penalty term, or Ridge.  This type of regularization applies  shrinkage to the coefficient estimates through a penalty term that has more penalty when the coefficients are high and less penalty when the coefficients are low.  This has the effect of implementing a trade-off between predictive accuracy, and size of the coefficients.  Additionally, the amount of this penalty is control through a tuning parameter, called lambda.  A large lambda will implement a large amount of shrinkage in the coefficients or penalty.  The opposite is true when lambda is small.  Identifying the amount of shrinkage can be tricky, so we can tune this parameter through a series of cross validated models. 

```{r include=FALSE}
set.seed (1)
library(glmnet)
#glmnet needs a model matrix
x<- model.matrix(donr~ ., data.train.std.c)[,-1]

#10-fold cross validated grid search for lambda
#alpha = 0 for L2/Ridge penalty
cv.out=cv.glmnet(x,data.train.std.c$donr, type.measure ="deviance", alpha = 0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
```

After completing a cross-validated grid search for lambda, we find that the optimal value is 0.291278.  This is the amount of shrinkage that we should apply to the coefficients to minimize error.  Fitting the logistic regression model with these parameters yields the following:

```{r include=FALSE}
valid_x<- model.matrix(donr~ ., data.valid.std.c)[,-1]
ridge.lr<- glmnet(x,data.train.std.c$donr, family = "binomial", alpha = 0, lambda = 0.0291278)
pred.ridge.lr=predict(ridge.lr, type = "response", newx=valid_x)

predl2 <- prediction(pred.ridge.lr,data.valid.std.c$donr)
perfl2 <- performance(predl2,"tpr","fpr")
plot(perfl2, main ="ROC Curve for Logistic Regression | L2")

```

A very similar ROC curve to the traditional logistic regression model ensues.  Let's see how these predictions are impacting our maximum profit.

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.lr.l2 <- cumsum(14.5*c.valid[order(pred.ridge.lr, decreasing=T)]-2)
plot(profit.lr.l2, main="Profit by Circulation") 
n.mail.valid.lr.l2 <- which.max(profit.lr.l2) 
c(n.mail.valid.lr.l2, max(profit.lr.l2)) 
```


```{r include=FALSE}
cutoff.lr.l2 <- sort(pred.ridge.lr, decreasing=T)[n.mail.valid.lr.l2+1] # set cutoff based on n.mail.valid
chat.valid.lr.l2 <- ifelse(pred.ridge.lr>cutoff.lr.l2, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lr.l2, c.valid) # classification table
```

The benefit is shown here, not from an increased amount of responders but from a much more efficient prediction of these responders.  We predict 974 responders out of a total 1,365 circulation, compared to the traditional logistic regression which predicted more responders 985, but through much more circulation 1,446.  Essentially those additional responders were not worth the cost to mail them. 

$$14.5* 974 (Responders) - 2* 1365(Total Mail Circulation) = 11,393 (Profit)$$

Let's examine the coefficient estimates to see how the shrinkage impacted the features.

```{r include=FALSE}
ridge_est<- ridge.lr$beta
lr_est<- model.lr$coefficients
ridge_est
lr_est
```

Feature | Logistic Regression with L2 | Logistic Regression
:-------|:------------|:---------------
reg1    |  0.25537000 | 0.56747345
reg2    |  0.66468907 | 1.21837833
reg3    | -0.08166385 | 0.01994871
reg4    | -0.10824003 | -0.0162182
home    |  0.66893955 | 1.12342549
chld    | -1.27598357 | -1.96467845
hinc    |  0.04977427 | 0.08435657
genf    | -0.02246717 | -0.02701978
wrat    |  0.44190367 | 0.69142161
avhv    |  0.04591216 | 0.07088138
incm    |  0.23463216 | 0.4747874
inca    |  0.07581667 | 0.03390637
plow    | -0.07537793 | -0.01545116
npro    |  0.13689277 | 0.09650472
tgif    |  0.24010934 | 0.44076473
lgif    | -0.02583325 | -0.12747192
rgif    | -0.02295262 | -0.02695524
tdon    | -0.16685811 | -0.23556973
tlag    | -0.29920010 | -0.45919368
agif    |  0.03652890 | 0.11737867
wrat_cat | 0.25527698 | 0.38291831

Comparing the matrix from the L2 logistic regression we can see this shrinking effect as all of the coefficients are smaller than the estimates from the traditional logistic regression.  



### Logistic Regression | L1 Regularization

Much like the L2 penalty for logistic regression that we just examined, the L1,  or lasso, regularization is another attempt at introducing a penalty to the feature set.  This penalty however is slightly different, as it doesn't just shrink the coefficients but it has the ability to zero out the estimates entirely and perform a type of feature selection.  This type of regularization can be particularly useful for models where only a few features are explaining the relationship with the response variable.  Just as we saw with the L2 regularization, we need to apply some magnitude of regularization and we do that through lambda again.  Instead of attempting to guess what lambda should be, we perform a cross validated grid search in order to find the optimal value of lambda.

```{r include=FALSE}
set.seed (1)
#10-fold cross validated grid search for lambda
#alpha = 1 for L1/Lasso penalty
cv.out=cv.glmnet(x,data.train.std.c$donr, type.measure ="deviance", alpha = 1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
```

We observe the optimal value for lambda after our cross validated grid search is 0.001591041.  This is not a significant amount of regularization, so the majority of the feature set appears to be playing a predictive role.  Next we fit a model with the optimal value of a lambda.

```{r include=FALSE}

lasso.lr<- glmnet(x,data.train.std.c$donr, family = "binomial", alpha = 1, lambda = 0.001591041)
pred.lasso.lr=predict(lasso.lr, type = "response", newx=valid_x)

predl1 <- prediction(pred.lasso.lr,data.valid.std.c$donr)
perfl1 <- performance(predl1,"tpr","fpr")
plot(perfl1, main ="ROC Curve for Logistic Regression | L1")

```

Nothing drastically different for this ROC curve, let's see how our profit calculation looks.  Given the fact that this ROC is similar, we should expect the profit to be similar. 

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.lr.l1 <- cumsum(14.5*c.valid[order(pred.lasso.lr, decreasing=T)]-2)
plot(profit.lr.l1, main="Profit by Circulation") 
n.mail.valid.lr.l1 <- which.max(profit.lr.l1) 
c(n.mail.valid.lr.l1, max(profit.lr.l1)) 
```


```{r include=FALSE}
cutoff.lr.l1 <- sort(pred.lasso.lr, decreasing=T)[n.mail.valid.lr.l1+1] # set cutoff based on n.mail.valid
chat.valid.lr.l1 <- ifelse(pred.lasso.lr>cutoff.lr.l1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lr.l1, c.valid) # classification table
```

The profit here is slightly worst, not because we couldn't predict responders, but because we took more circulation to do it.  We had a clue that this performance would be slightly worse when we ran our cross validation for lambda.  Since lambda was very small, we were applying very little regularization which would be a clue to us that a lot of the features need to play a role in the prediction.  As such, whenever the majority of th feature set explains the behavior of the response, the L2 regularization will outperform the L1 regularization. 

$$14.5* 975 (Responders) - 2* 1379(Total Mail Circulation) = 11,379.50 (Profit)$$

Lastly, let's compare the coefficients from this model and compare them to the traditional logistic regression model and logistic regression model with L2 regularization.

```{r include=FALSE}
lasso_est<- lasso.lr$beta
lasso_est

```

Feature | Logistic Regression with L2 | Logistic Regression | Logistic Regression with L1
:-------|:----------------------------|:--------------------|:----------------------------
reg1    |  0.25537000 | 0.56747345 | 0.520325884
reg2    |  0.66468907 | 1.21837833 | 1.146254944
reg3    | -0.08166385 | 0.01994871 | NA
reg4    | -0.10824003 | -0.0162182 | -0.018568756
home    |  0.66893955 | 1.12342549 | 1.063425157
chld    | -1.27598357 | -1.96467845 | -1.887569094
hinc    |  0.04977427 | 0.08435657 | 0.064668501
genf    | -0.02246717 | -0.02701978 | -0.012185088
wrat    |  0.44190367 | 0.69142161 | 0.662406042
avhv    |  0.04591216 | 0.07088138 | 0.062916663
incm    |  0.23463216 | 0.4747874 | 0.457648174
inca    |  0.07581667 | 0.03390637 | 0.031112682
plow    | -0.07537793 | -0.01545116 | -0.009518454
npro    |  0.13689277 | 0.09650472 | 0.097311631
tgif    |  0.24010934 | 0.44076473 | 0.406723055
lgif    | -0.02583325 | -0.12747192 | -0.054159546
rgif    | -0.02295262 | -0.02695524 | NA
tdon    | -0.16685811 | -0.23556973 | -0.215066455
tlag    | -0.29920010 | -0.45919368 | -0.431476075
agif    |  0.03652890 | 0.11737867 | 0.023587496
wrat_cat | 0.25527698 | 0.38291831 | 0.335687103

Observing the L1 estimates we see that two of the features we're shrunk all the way to 0 and reduced the dimensions of our model by 2.  This model was slightly worse performing than the other two versions of logistic regression most likely because it appears that the majority of features are providing some predictive benefit.  

Next we will explore a non-parametric approach to classification, k-Nearest Neighbors.

### k-Nearest Neighbors Classification (KNN)

The KNN classification approach is very flexible and as such must be used carefully.  KNN iterates through a set of training data and finds the k nearest neighbors. Once the k neighbors have been identified, it applies bayes rule to classifiy a test observation with the largest class probability.  Since the value of k is complete heuristic and up to the analyst, we must be careful to validate the value of k that we use.  

```{r include=FALSE}
data.train.std.c
library(class)
data.train.std.knn <- subset(data.train.std.c, select = -c(donr))
data.valid.std.knn <- subset(data.valid.std.c, select = -c(donr))

knn.pred1=knn(data.train.std.knn,data.valid.std.knn,data.train.std.c$donr,k=1, prob = TRUE)
table(knn.pred1,data.valid.std.c$donr)

knn.pred3=knn(data.train.std.knn,data.valid.std.knn,data.train.std.c$donr,k=3, prob = TRUE)
table(knn.pred3,data.valid.std.c$donr)

knn.pred5=knn(data.train.std.knn,data.valid.std.knn,data.train.std.c$donr,k=5, prob = TRUE)
table(knn.pred5,data.valid.std.c$donr)

knn.pred10=knn(data.train.std.knn,data.valid.std.knn,data.train.std.c$donr,k=10, prob = TRUE)
table(knn.pred10,data.valid.std.c$donr)

knn.pred50=knn(data.train.std.knn,data.valid.std.knn,data.train.std.c$donr,k=50, prob = TRUE)
table(knn.pred50,data.valid.std.c$donr)

knn.pred100=knn(data.train.std.knn,data.valid.std.knn,data.train.std.c$donr,k=100, prob = TRUE)
table(knn.pred100,data.valid.std.c$donr)

```


Assesing a few different values of k, we notice that our prediction accuracy for the responders grows as K grows. 

% of responders predicted 

* k = 1 | 83.38%
* k = 3 | 89.38%
* k = 5 | 91.39%
* k = 10 | 92.79%
* k = 50 | 95.30%
* k = 100 | 96.09%

However, even though we are predicting more and more of the responders we are also classify more wrong predictions for responders.  

Responder prediction accuracy

* k = 1 | 74.44%
* k = 3 | 75.67%
* k = 5 | 76.03%
* k = 10 | 75.06%
* k = 50 | 70.87%
* k = 100 | 69.41%

These two opposing factors impact the maximum profit.  The maximum profit for each value of k is as follows:

* k = 1 | $9,840.50
* k = 3 | $10,588.50
* k = 5 | $10,834.50
* k = 10 | $10,971.50
* k = 50 | $11,114.00
* k = 100 | $11,154.00

Interestingly, even though the k=100 model is over-predicting the responders by a wide amount, it still produces the largest profit.  However, all of these models are worse than the LDA, QDA, and Logistic Regression model.  It would appear that this model is not best suited for this feature set.

Next we will attempt some tree-based predictions that will attempt to model the classification through splitting the data set based on the features that produce the best improvement in classification. 

## Random Forest Classification

An alternative approach to creating classifications is to create a decision tree which splits up the dataset based on the features that do the best job of classifying the observations after the split.  This analysis of the best classification can be done by assess the misclassification in the subsequent groupings after the split, or it can be calculated using some metric of impurity.  The impurity in the nodes after a split is how we will assess our decision trees. 

However simply building one decision tree might not be robust enough to adapt to the complexity of the dataset or the additional datasets for which we wish to generate predictions.  Random forests are a collection of these decision trees and they are created in an interesting way.  Random forests use a technique called bagging, or bootstrap aggregation, which resamples the same dataset repeatedly to generate multiple decision trees.  However, it doesn't just simply re-sample the dataset over and over again, it using a technique called decorrelated bagging in which it forces different features to be removed from the tree building process.  While this might seem strange that features could be removed, especially highly predictive ones, but the benefit of this in tandom with repeated resampling is teasing out smaller effects that would be hidden by building similar trees over and over. The amount of decorrelation is adjusted through the amount of features used in each tree, or m.  The suggested value of m is the square root of p, or the total count of features.  Our feature set is made up of 21 features, so the optimal value for m in our dataset is ~4.58.  As a result, we will set m to 5. 

```{r include=FALSE}

library(randomForest)
#random forest with m=5
rf=randomForest(data.train.std.knn, as.factor(data.train.std.c$donr), mtry=5,prox = TRUE, importance = TRUE)
pred.rf<- predict(rf, data.valid.std.knn, type = "prob")
pred.rf[,2]

predrf <- prediction(pred.rf[,2],data.valid.std.c$donr)
perfrf <- performance(predrf,"tpr","fpr")
plot(perfrf, main ="ROC Curve for Random Forest")

```

It appears that the ROC curve for the random forest is pulled slightly to the upper left portion of the graph, indicating better predictive performane.  Let's plot our best logistic regression curve on the same plot (in a red line) and see how they compare.

```{r include=FALSE}

plot(perfl2, main = "ROC Curve for Logistic Regression | L2 and Random Forest", col = 'red')
plot(perfrf, add = TRUE)

```

It's clear to see that the random forest is providing a substantial improvement in performance.  Let's calculate the profit from the random forest to see how this increase in accuracy produces a higher profit. 

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.rf <- cumsum(14.5*c.valid[order(pred.rf[,2], decreasing=T)]-2)
plot(profit.rf, main="Profit by Circulation") 
n.mail.valid.rf <- which.max(profit.rf) 
c(n.mail.valid.rf, max(profit.rf)) 
```

```{r include=FALSE}
cutoff.rf <- sort(pred.rf[,2], decreasing=T)[n.mail.valid.rf+1] # set cutoff based on n.mail.valid
chat.valid.rf <- ifelse(pred.rf[,2]>cutoff.rf, 1, 0) # mail to everyone above the cutoff
table(chat.valid.rf, c.valid) # classification table
```

It's obvious to see that the performance of this model is significantly better.  We achieve the highest profit so far by a wide margin, at $11,770.5.  This is because we can predict basically the same amount of responders, 985 of them, with significantly less circulation, only need 1256 mailings. 

$$14.5* 985 (Responders) - 2* 1256(Total Mail Circulation) = 11,770.50 (Profit)$$
One of the nice things about running a random forest is that you can assess which features were on average driving the biggest reduction in node impurity as the trees were built.  This is a powerful clue into which features are the the most predictive.   


```{r include=FALSE}

#plotting feature importance
varImpPlot (rf)

```

As we observed with our EDA, children in the household was very detrimental to donations and we can see here that it's clearly the most important feature in predicting donors. 

Another type of decision tree model we can pursue is a gradient boosted tree

### Gradient Boosted Trees (GBM)

As we saw with the random forest, multiple trees were grown from repeated sampling of the dataset.  Boosting takes a slightly different approach where trees are grown sequentially, meaning that they use the previous tree to grow the subsequent tree. Since the trees are grow sequentially, they utilize the residuals from the previous tree and grow "slowly" (Section 8.2.3, James, Witten, Hastie, Tibshirani).


```{r include=FALSE}

install.packages("gbm")
library(gbm)
set.seed (1)
boost=gbm.fit(data.train.std.knn, data.train.std.c$donr,distribution="bernoulli",n.trees=1000, interaction.depth=20)
pred.boost=predict(boost,newdata=data.valid.std.knn, n.trees=100)

predboost <- prediction(pred.boost,data.valid.std.c$donr)
perfboost <- performance(predboost,"tpr","fpr")
plot(perfboost, main ="ROC Curve for Gradient Boosted Trees")

```


```{r include=FALSE}

plot(perfboost, main = "ROC Curve for Logistic Regression | L2, Random Forest, and GBM", col = 'red')
plot(perfrf, add = TRUE, col = 'green')
plot(perfl2, add = TRUE)

```


Here we can see that the GBM model is performing slightly better than the logistic regression model and slightly worse than the random forest. We should expect the maximum profit to be in between those two models.

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.boost <- cumsum(14.5*c.valid[order(pred.boost, decreasing=T)]-2)
plot(profit.boost, main="Profit by Circulation") 
n.mail.valid.boost <- which.max(profit.boost) 
c(n.mail.valid.boost, max(profit.boost)) 
```


```{r include=FALSE}
cutoff.boost <- sort(pred.boost, decreasing=T)[n.mail.valid.boost+1] # set cutoff based on n.mail.valid
chat.valid.boost <- ifelse(pred.boost>cutoff.boost, 1, 0) # mail to everyone above the cutoff
table(chat.valid.boost, c.valid) # classification table
```

$$14.5* 995 (Responders) - 2* 1412(Total Mail Circulation) = 11,603.50 (Profit)$$

The GBM is doing a great job at predicting responders, our highest total yet at 995.  However, it comes at a step cost as we need to mail 1,412 potential donors to get those responses.  As a result the total profit is not as high as the random forest which did a better job at optimizing the responders and costs.


As we did we the random forest, we can evaluate the feature importance. 

```{r include=FALSE}

#plotting feature importance
summary(boost)

```

Again we notice a the children in household being a powerful feature to use for donor prediction. 

The last model that we will attempt is a support vector machine.

### Support Vector Machine | Linear

The general concept behind a support vector machine is to create a hyperplane that seperates the two classes.  Observations on one side of the hyperplane are a part of one class, and the other side represents the other class.  The observations that form this hyperplane are called the support vectors, and they also form the margin, or gap between the two sets of classes.  It may be desired to define a hyperplane that does not perfectly seperate the classes, as there are dangers of overfitting.  This slack can be introduced in the model which allows for a number of observations to be misclassified.  This is often desirable as it can lead to better performance on different datasets.  In order to identify what this optimal margin should be, we can tune the parameter for margin (cost) with cross fold validation.

Additionally, there are a number of kernel functions we can use to create our decision boundary and classification.  The first kernel we will use is a linear one, which models a linear decision boundary.

```{r include=FALSE}
library(ROCR)
library(e1071)
dat=data.frame(x=data.train.std.knn, y=as.factor(data.train.std.c$donr))

tune.out=tune(svm,y~.,data=dat,kernel="linear",
                ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))

summary(tune.out)

#fit using best model from tune
bestmod=tune.out$best.model
summary(bestmod)

```

Now that we have the optimal value the cost, we fit the model and assess the performance on the validation dataset.

```{r include=FALSE}
testdat=data.frame(x=data.valid.std.knn, y=as.factor(data.valid.std.c$donr))

svmfit=svm(y~., data=testdat, kernel="linear", cost=.01, gamma = 0.04761905, scale=FALSE, decision.values = T, probability = TRUE)
fitted=attributes(predict(svmfit,testdat,
                          decision.values=TRUE))$decision.values

rocplot(fitted, testdat$y, main="ROC of Linear SVM")
```

This ROC doesn't appear to be as impressive as what we acheived with the random forest or gradient boosted trees.  Let's evaluate the profit and see if our assumptions are correct. 

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.svm <- cumsum(14.5*c.valid[order(fitted, decreasing=T)]-2)
plot(profit.svm, main="Profit by Circulation") 
n.mail.valid.svm <- which.max(profit.svm) 
c(n.mail.valid.svm, max(profit.svm)) 
```

```{r include=FALSE}
cutoff.svm <- sort(fitted, decreasing=T)[n.mail.valid.svm+1] # set cutoff based on n.mail.valid
chat.valid.svm <- ifelse(fitted>cutoff.svm, 1, 0) # mail to everyone above the cutoff
table(chat.valid.svm, c.valid) # classification table
```

As we suspected, the profit is only $11,373.50 from a circulation quantity of 1,440.  We are able to classify the majority of responders correctly, but we have too many errors which increase our circulation and decrease total profit.

$$14.5* 983 (Responders) - 2* 1440(Total Mail Circulation) = 11,373.50 (Profit)$$

Next let's create a support vector machine with a differnt kernel, one that allows for non-linear decision boundaries.  The kernel we will use next is the RBF, or radial basis function. 

### Support Vector Machine | Radial Basis Function (RBF)

```{r include=FALSE}

tune.out.rbf=tune(svm,y~.,data=dat,kernel="radial",
                ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))

summary(tune.out.rbf)

#fit using best model from tune
bestmod.rbf=tune.out.rbf$best.model
summary(bestmod.rbf)

```

```{r include=FALSE}

svmfit.rbf=svm(y~., data=testdat, kernel="radial", cost=5, gamma = 0.04761905, scale=FALSE, decision.values = T, probability = TRUE)
fitted.rbf=attributes(predict(svmfit.rbf,testdat,
                          decision.values=TRUE))$decision.values

rocplot(fitted.rbf, testdat$y, main="ROC of RBF SVM")
```

This ROC is very promising as we notice a strong pull to the upper left portion of the graph. Let's compare this to the best model we have had so far, the random forest model.

```{r include=FALSE}
rocplot(fitted.rbf, testdat$y, main="ROC of RBF SVM and Random Forest", col = 'red')
plot(perfrf, add = TRUE)
```

Visually we can see superior performance for the SVM with RBF kernel.  With this type of increase in prediction performance, we would expect significantly better maximum profit.

```{r include=FALSE}
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.svm.rbf <- cumsum(14.5*c.valid[order(fitted.rbf, decreasing=T)]-2)
plot(profit.svm.rbf, main="Profit by Circulation") 
n.mail.valid.svm.rbf <- which.max(profit.svm.rbf) 
c(n.mail.valid.svm.rbf, max(profit.svm.rbf)) 
```


```{r include=FALSE}
cutoff.svm.rbf <- sort(fitted.rbf, decreasing=T)[n.mail.valid.svm.rbf+1] # set cutoff based on n.mail.valid
chat.valid.svm.rbf <- ifelse(fitted.rbf>cutoff.svm.rbf, 1, 0) # mail to everyone above the cutoff
table(chat.valid.svm.rbf, c.valid) # classification table
```

$$14.5* 992 (Responders) - 2* 1124(Total Mail Circulation) = 12,136 (Profit)$$

The SVM with RBF kernel does a tremendoes job at predicting response and does it very accurately.  As a result, we get the majority of the responders and don't waste a lot of circulation.  This is evidence by the largest profit we have attained thus far. 

### Classification Summary

The criteria for which we are going to evaluate all of the classification models is to see which model produces the highest profit on the validation dataset.  Listed below are the profit values for each of the models:

Model | Profit
:---------------------|:------------------------:
SVM, Radial Basis Function | $12,136.00
Random Forest | $11,770.50
Gradient Boosted Trees | $11,603.50
Logsitic Regression, L2 Regularization | $11,393
Logistic Regression | $11,390.50
Logsitic Regression, L1 Regularization | $11,379.50
SVM, Linear | $11,373.50
Linear Discriminant Analysis | $11,355.50
Quadractic Discriminant Analysis | $11,205
k-NN Classification | $11,154.00

The choice is an easy one here, as the SVM, radial basis function outperforms all of the models by a significant amount.  The rest of the models perform similarly the the exception of the k-NN classification and QDA. 

Now that we have created our classification model for predicting those that will donate, next we will create a model to predict the amount of donations for those that donate.

## Prediction Model

There are a variety of different models developed that can be used to predict a quantitative variable, and in our case that is the donation amount.  We will develop the following models for our prediction of donation amount: 

* Linear Regression
* Linear Regression | Best Subset Model
* Linear Regression | L2 Regularization
* Linear Regression | L1 Regularization
* Principal Components Regression
* Random Forest Regression
* Gradient Boosted Tree Regression

As we did with the classification models, we will learn the models on our training dataset and predict values on our validation dataset.  We will use the mean squared error of the validation dataset in order to rank which models perform the best. 

### Linear Regression

Linear regression is one of the most popular and commonly used techniques to predict quantitative values.  We use a technique called ordinary least squares where we fit a linear line that produces the lowest total value of the squared value of each observation distance from our line.  This technique is reliant on a variety of assumptions and hence can be very robust when those assumptions are met, and not reliable when they are not.  

```{r include=FALSE}

#Fit an OLS regression model
model.lm <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif + wrat, 
                data.train.std.y)

pred.valid.lm <- predict(model.lm, newdata = data.valid.std.y) # validation predictions

mean((y.valid - pred.valid.lm)^2) # mean prediction error

sd((y.valid - pred.valid.lm)^2)/sqrt(n.valid.y) # std error


```

Fitting a linear regression model on the training dataset and predicting donation amount of the validation dataset, we find a mean prediction error of 1.5564 and a standard error of our predictions is 0.161215.  The standard error of our predictions is helpful in identifying how accurate our predictions are.  For example if we over predict and under predict wildly, the mean squared error might not give us this insight if they cancel each other out.  Using the standard error helps evalaute how tight the prediction bands are.

As we mentioned earlier, linear regression is robust if the assumptions are met.  In order to meet the assumptions our residuals need to be independently and identically distributed.  So we can plot our residuals against our fitted values to see if this is the case.  

```{r include=FALSE}

layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(model.lm)
```

The graphic in the upper right is showing us that for the most part, the residuals are random and we are seeing no pattern.  The QQ plot confirms this, but we do see some skewness as the residuals are not all following on the dotted line.  These plots would indicate to us that using this type of regression is acceptable and as a result the coefficients and ensuing predictions will be for the most part reliable. 

However, this model is including every variable and in hopes of using a simpler model because simple tend to "travel well" and predict more types of datasets we will examine a few types of dimension reduction and regularization. 

### Linear Regression | Best Subset Model

In hopes of finding a simpler subset model that predicts as well as the full model, it's possible to create a model with every combination of variables.  The approach is called the best subset model, and it is computationally very expensive since the amount of potential models are 2**p.  In our example, we have 21 features, so there are 2,097,152 different model combinations.  We will attempt this model because our dataset is not too large.

```{r include=FALSE}

#install.packages("leaps")
library(leaps)
bss= regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif + wrat, 
                data.train.std.y)

reg.summary<- summary(bss)
reg.summary 

par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
     type="l")

plot(reg.summary$adjr2 ,xlab="Number of Variables ",
     ylab="Adjusted RSq",type="l")

plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')

plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",
     type='l')

```

By examining the summary, we can see that the best subset model includes only 8 of the 21 features.  The features that are included have a star in the row for that feature.  The 8 features are:

* lgif
* reg4
* chld
* hinc
* reg3
* rgif
* agif
* tgif

After these 8 features are included, we don't see any meaningful difference in adjusted r-squared, RSS, BIC, or Mallows' Cp. A strong indicator that we shouldn't include any more features, because the prediction won't get any better but it becomes more complex. Now that we have our simpler model, we can predict our validation dataset with a model that only includes these 8 features.

```{r include=FALSE}

model.bss<- lm(damt ~ lgif + reg4 + chld + hinc + reg3 + rgif + agif + tgif, data.train.std.y)
pred.valid.bss <- predict(model.bss, newdata = data.valid.std.y) # validation predictions

mean((y.valid - pred.valid.bss)^2) # mean prediction error

sd((y.valid - pred.valid.bss)^2)/sqrt(n.valid.y) # std error

```

The mean squared error is slightly larger than the full model, but it's acheived with significantly less features.  The standard error is essentially the same, which again is promising considering the significantly smaller feature set.  


```{r include=FALSE}

layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(model.bss)

```

Plotting the residuals and fitted values we see very similar behavior as we did with the full model.  For the most part the residuals are random and the predictions don't appear have a pattern based on the size of the fitted value.

While this model was simpler, and in theory a model that would accomodate additional datasets than a more complicated one, it didn't predict as well on our validation dataset and therefore will not chose this model.

### Linear Regression | L2 Regularization


```{r include=FALSE}


```

# See below for saving chat.test into a file for submission

##### PREDICTION MODELING ######

# Least squares regression

model.ls1 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls1 <- predict(model.ls1, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls1)^2) # mean prediction error
# 1.867523
sd((y.valid - pred.valid.ls1)^2)/sqrt(n.valid.y) # std error
# 0.1696615

# drop wrat for illustrative purposes
model.ls2 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls2 <- predict(model.ls2, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls2)^2) # mean prediction error
# 1.867433
sd((y.valid - pred.valid.ls2)^2)/sqrt(n.valid.y) # std error
# 0.1696498

# Results

# MPE  Model
# 1.867523 LS1
# 1.867433 LS2

# select model.ls2 since it has minimum mean prediction error in the validation sample

yhat.test <- predict(model.ls2, newdata = data.test.std) # test predictions




# FINAL RESULTS

# Save final results for both classification and regression

length(chat.test) # check length = 2007
length(yhat.test) # check length = 2007
chat.test[1:10] # check this consists of 0s and 1s
yhat.test[1:10] # check this consists of plausible predictions of damt

ip <- data.frame(chat=chat.test, yhat=yhat.test) # data frame with two variables: chat and yhat
write.csv(ip, file="ABC.csv", row.names=FALSE) # use your initials for the file name

# submit the csv file in Angel for evaluation based on actual test donr and damt values
