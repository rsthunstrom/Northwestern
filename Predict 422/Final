---
title: "PRED422_Final"
author: "Reed Thunstrom"
subtitle: Predict 422 | Section 58
output: word_document
---

## Introduction

The purpose of this report is to aid a charitable organization with their direct marketing efforts.  The charitable organiziation has a list of potential donors with the history of the past donations, this will be the dataset used for the analysis.  The data set consists of 21 features or attributes about the potential donors as well as if the donor has donated in the past, and if so how much.  We will separate the file into a training dataset used to learn the models and a validation dataset which will be used to assess the predictive accuracy of each model.  This analysis will consists of two parts: the first is a classification model used to predict if a target of the direct marketing will donate and a regression model to predict the amount of donation if they were to donate.  These two components will be invaluable to the charity organiziation as they will be able to optimize their mailing for maximum net donations after considering the cost of the mailing.  

Each donor, on average, donates \$14.50 and each piece of direct marketing costs \$2.00. We will use these two metrics in our analysis to define an optimal mailing which will produce the maximum net donations for the charitable organization.  Now that we have describe the goal of the analysis, let's start to expolore the data that is available. 

## Exploratory Data Analysis (EDA)

First let's load in our data and take a look at the first few rows of data. 

```{r setup, include=FALSE}

charity <- read.csv('/Users/z013nx1/Documents/charity.csv') # load the "charity.csv" file
head(charity)

```

We notice that we have 24 columns, 1 ID, 20 features, 1 classification dependent variable (DONR), 1 regression dependent variable(DAMT), and 1 column seperating the file into our training, testing, and validation datasets.  Let's split up of file into the three modeling subsets and start to explore the feature set.

```{r include=FALSE}
data.train <- charity[charity$part=="train",]
data.valid <- charity[charity$part=="valid",]
data.test <- charity[charity$part=="test",]
length(data.train[,1])
length(data.valid[,1])
length(data.test[,1])
```

The training dataset has 3,984 observations and we will validate our models on the validation set which consists of 2,018 observations.  The testing set that we will deploy our final models on has 2,007 observations and we do not have any values for DONR, the binary classification of donor/non-donor or DAMT, the amount of donation for a donor.

Feature | Description
-----------------------------|-------------------------------------------------
ID number | Number used as an ID
REG1 | Potential donor located in region 1
REG2 | Potential donor located in region 2
REG3 | Potential donor located in region 3
REG4 | Potential donor located in region 4
HOME | 1 = homeowner, 0 = not a homeowner
CHLD | Number of children
HINC | Household income (7 categories)
GENF | Gender (0 = Male, 1 = Female)
WRAT | Wealth Rating (0 lowest - 9 highest)
AVHV | Average Home Value in potential donor's neighborhood in $ thousands
INCM | Median Family Income in potential donor's neighborhood in $ thousands
INCA | Average Family Income in potential donor's neighborhood in $ thousands
PLOW | Percent categorized as “low income” in potential donor's neighborhood
NPRO | Lifetime number of promotions received to date
TGIF | Dollar amount of lifetime gifts to date
LGIF | Dollar amount of largest gift to date
RGIF | Dollar amount of most recent gift
TDON | Number of months since last donation
TLAG | Number of months between first and second gift
AGIF | Average dollar amount of gifts to date
DONR | Classification Response Variable (1 = Donor, 0 = Non-donor)
DAMT | Prediction Response Variable (Donation Amount in $).

Now that our datasets are split, let's examine the feature set of the training dataset.  First let's plot a correlation matrix to see the relationship of each feature with each, but also how each individual feature is correlated with our response variables. 

```{r include=FALSE}
#remove ID 
data.training<- data.train[, 2:24]

#install.packages("corrgram")
library(corrgram)
corrgram(data.training, order=NULL, lower.panel=panel.shade,
  upper.panel=NULL, text.panel=panel.txt,
  main="Training Data - Correlogram")

```

In this graphic, the heavily shaded blue boxes are indicating high positive correlations and the heavily shaded red boxes are a representation of a negative correlation.  This correlogram shows us a few things we might except, for example if someone donates, their donation amount is positive.  Additionaly, the regions have a negative correlation since each potential donor can only be located in one region.  The features about income are related to donor and donor amount as well.  We notice a positive correlatoin with average and median income as well as a negative correlation the percent of low income families in the donor's neighborhood.  The heaviest shaded areas for donor and donor amount appear to be the feature indicating home ownership and the number of children in the household. 


```{r include=FALSE}
mylm<- lm(damt ~ chld, data = data.training)
reg_line<- mylm$coefficients

plot(x = data.training$chld, y = data.training$damt, xlab = "# - Children", ylab = "Donation Amount - $", main = "Donation Amount x Children in Household")
abline(reg_line, col = "green")
grid()

print (reg_line)

```
# OBJECTIVE: A charitable organization wishes to develop a machine learning
# model to improve the cost-effectiveness of their direct marketing campaigns
# to previous donors.

# 1) Develop a classification model using data from the most recent campaign that
# can effectively capture likely donors so that the expected net profit is maximized.

# 2) Develop a prediction model to predict donation amounts for donors - the data
# for this will consist of the records for donors only.


# predictor transformations


# add further transformations if desired
# for example, some statistical methods can struggle when predictors are highly skewed

# set up data for analysis

data.train <- charity.t[charity$part=="train",]
x.train <- data.train[,2:21]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995

data.valid <- charity.t[charity$part=="valid",]
x.valid <- data.valid[,2:21]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999

data.test <- charity.t[charity$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,2:21]

x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)


##### CLASSIFICATION MODELING ######

# linear discriminant analysis

library(MASS)

model.lda1 <- lda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                  data.train.std.c) # include additional terms on the fly using I()

# Note: strictly speaking, LDA should not be used with qualitative predictors,
# but in practice it often is if the goal is simply to find a good predictive model

post.valid.lda1 <- predict(model.lda1, data.valid.std.c)$posterior[,2] # n.valid.c post probs

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2

profit.lda1 <- cumsum(14.5*c.valid[order(post.valid.lda1, decreasing=T)]-2)
plot(profit.lda1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.lda1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.lda1)) # report number of mailings and maximum profit
# 1329.0 11624.5

cutoff.lda1 <- sort(post.valid.lda1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.lda1 <- ifelse(post.valid.lda1>cutoff.lda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda1, c.valid) # classification table
#               c.valid
#chat.valid.lda1   0   1
#              0 675  14
#              1 344 985
# check n.mail.valid = 344+985 = 1329
# check profit = 14.5*985-2*1329 = 11624.5

# logistic regression

model.log1 <- glm(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + 
                    avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                  data.train.std.c, family=binomial("logit"))

post.valid.log1 <- predict(model.log1, data.valid.std.c, type="response") # n.valid post probs

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2

profit.log1 <- cumsum(14.5*c.valid[order(post.valid.log1, decreasing=T)]-2)
plot(profit.log1) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.log1) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.log1)) # report number of mailings and maximum profit
# 1291.0 11642.5

cutoff.log1 <- sort(post.valid.log1, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.log1 <- ifelse(post.valid.log1>cutoff.log1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.log1, c.valid) # classification table
#               c.valid
#chat.valid.log1   0   1
#              0 709  18
#              1 310 981
# check n.mail.valid = 310+981 = 1291
# check profit = 14.5*981-2*1291 = 11642.5

# Results

# n.mail Profit  Model
# 1329   11624.5 LDA1
# 1291   11642.5 Log1

# select model.log1 since it has maximum profit in the validation sample

post.test <- predict(model.log1, data.test.std, type="response") # post probs for test data

# Oversampling adjustment for calculating number of mailings for test set

n.mail.valid <- which.max(profit.log1)
tr.rate <- .1 # typical response rate is .1
vr.rate <- .5 # whereas validation response rate is .5
adj.test.1 <- (n.mail.valid/n.valid.c)/(vr.rate/tr.rate) # adjustment for mail yes
adj.test.0 <- ((n.valid.c-n.mail.valid)/n.valid.c)/((1-vr.rate)/(1-tr.rate)) # adjustment for mail no
adj.test <- adj.test.1/(adj.test.1+adj.test.0) # scale into a proportion
n.mail.test <- round(n.test*adj.test, 0) # calculate number of mailings for test set

cutoff.test <- sort(post.test, decreasing=T)[n.mail.test+1] # set cutoff based on n.mail.test
chat.test <- ifelse(post.test>cutoff.test, 1, 0) # mail to everyone above the cutoff
table(chat.test)
#    0    1 
# 1676  331
# based on this model we'll mail to the 331 highest posterior probabilities

# See below for saving chat.test into a file for submission



##### PREDICTION MODELING ######

# Least squares regression

model.ls1 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls1 <- predict(model.ls1, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls1)^2) # mean prediction error
# 1.867523
sd((y.valid - pred.valid.ls1)^2)/sqrt(n.valid.y) # std error
# 0.1696615

# drop wrat for illustrative purposes
model.ls2 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls2 <- predict(model.ls2, newdata = data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls2)^2) # mean prediction error
# 1.867433
sd((y.valid - pred.valid.ls2)^2)/sqrt(n.valid.y) # std error
# 0.1696498

# Results

# MPE  Model
# 1.867523 LS1
# 1.867433 LS2

# select model.ls2 since it has minimum mean prediction error in the validation sample

yhat.test <- predict(model.ls2, newdata = data.test.std) # test predictions




# FINAL RESULTS

# Save final results for both classification and regression

length(chat.test) # check length = 2007
length(yhat.test) # check length = 2007
chat.test[1:10] # check this consists of 0s and 1s
yhat.test[1:10] # check this consists of plausible predictions of damt

ip <- data.frame(chat=chat.test, yhat=yhat.test) # data frame with two variables: chat and yhat
write.csv(ip, file="ABC.csv", row.names=FALSE) # use your initials for the file name

# submit the csv file in Angel for evaluation based on actual test donr and damt values
